{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2bb3b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# необхідні бібліотеки\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f07c9b",
   "metadata": {},
   "source": [
    "## Завантаження даних \n",
    "(вихідний файл використовував табуляцію замість коми як роздільник, тому додано параметр sep='\\tʼ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e99dc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df.csv', sep='\\t')\n",
    "delay_df = pd.read_csv('delay_df.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6962bd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>num_1</th>\n",
       "      <th>num_2</th>\n",
       "      <th>num_3</th>\n",
       "      <th>num_4</th>\n",
       "      <th>num_5</th>\n",
       "      <th>num_6</th>\n",
       "      <th>num_7</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>num_8</th>\n",
       "      <th>...</th>\n",
       "      <th>num_413</th>\n",
       "      <th>cat_132</th>\n",
       "      <th>cat_133</th>\n",
       "      <th>num_414</th>\n",
       "      <th>num_415</th>\n",
       "      <th>num_416</th>\n",
       "      <th>cat_134</th>\n",
       "      <th>cat_135</th>\n",
       "      <th>id</th>\n",
       "      <th>gb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1377.3</td>\n",
       "      <td>3712.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>2409.0</td>\n",
       "      <td>1281.7</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>195.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11327.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28162496.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>565.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>357.2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>170.0</td>\n",
       "      <td>...</td>\n",
       "      <td>168.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>989383.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>150.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2294.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>107.6</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>196.0</td>\n",
       "      <td>...</td>\n",
       "      <td>209.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>87444.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>11.4</td>\n",
       "      <td>178.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1236.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350.6</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>56.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>297608.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>5372.0</td>\n",
       "      <td>3386.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6614247.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 554 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   num_1   num_2  num_3   num_4   num_5   num_6  num_7  cat_1  \\\n",
       "0           0  1377.3  3712.9    NaN  1303.0  2409.0  1281.7   43.0      1   \n",
       "1           1    20.0    13.4    NaN   565.0     NaN   357.2   19.0      1   \n",
       "2           4   150.6     1.8    NaN  2294.0    15.0   107.6   76.0      1   \n",
       "3           5    11.4   178.0    NaN  1236.0     NaN   350.6   41.0      1   \n",
       "4           6  5372.0  3386.0    NaN  1340.0     NaN    14.0   45.0      1   \n",
       "\n",
       "   num_8  ...  num_413  cat_132  cat_133      num_414  num_415  num_416  \\\n",
       "0  195.0  ...  11327.5        1        1  28162496.65      0.0     0.07   \n",
       "1  170.0  ...    168.6        1        1    989383.82      0.0     0.56   \n",
       "2  196.0  ...    209.1        1        1     87444.51      0.0     0.01   \n",
       "3  117.0  ...     56.9        1        1    297608.00      0.0     0.08   \n",
       "4    9.0  ...     31.0        1        1   6614247.89      0.0     0.20   \n",
       "\n",
       "   cat_134  cat_135  id  gb  \n",
       "0        1        1   1   0  \n",
       "1        2        1   2   0  \n",
       "2        3        1   3   0  \n",
       "3        4        1   4   0  \n",
       "4        1        1   5   0  \n",
       "\n",
       "[5 rows x 554 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe243b6c",
   "metadata": {},
   "source": [
    "відобразимо всі колонки, щоб первісно проглянути, скільки їх, які вони мають значення, чи є пропущені тощо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f5a15e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>num_1</th>\n",
       "      <th>num_2</th>\n",
       "      <th>num_3</th>\n",
       "      <th>num_4</th>\n",
       "      <th>num_5</th>\n",
       "      <th>num_6</th>\n",
       "      <th>num_7</th>\n",
       "      <th>cat_1</th>\n",
       "      <th>num_8</th>\n",
       "      <th>num_9</th>\n",
       "      <th>num_10</th>\n",
       "      <th>num_11</th>\n",
       "      <th>num_12</th>\n",
       "      <th>num_13</th>\n",
       "      <th>num_14</th>\n",
       "      <th>num_15</th>\n",
       "      <th>num_16</th>\n",
       "      <th>num_17</th>\n",
       "      <th>num_18</th>\n",
       "      <th>num_19</th>\n",
       "      <th>num_20</th>\n",
       "      <th>num_21</th>\n",
       "      <th>num_22</th>\n",
       "      <th>num_23</th>\n",
       "      <th>cat_2</th>\n",
       "      <th>num_24</th>\n",
       "      <th>cat_3</th>\n",
       "      <th>cat_4</th>\n",
       "      <th>num_25</th>\n",
       "      <th>num_26</th>\n",
       "      <th>num_27</th>\n",
       "      <th>cat_5</th>\n",
       "      <th>num_28</th>\n",
       "      <th>num_29</th>\n",
       "      <th>cat_6</th>\n",
       "      <th>cat_7</th>\n",
       "      <th>cat_8</th>\n",
       "      <th>num_30</th>\n",
       "      <th>cat_9</th>\n",
       "      <th>cat_10</th>\n",
       "      <th>num_31</th>\n",
       "      <th>num_32</th>\n",
       "      <th>num_33</th>\n",
       "      <th>num_34</th>\n",
       "      <th>num_35</th>\n",
       "      <th>num_36</th>\n",
       "      <th>num_37</th>\n",
       "      <th>num_38</th>\n",
       "      <th>num_39</th>\n",
       "      <th>num_40</th>\n",
       "      <th>num_41</th>\n",
       "      <th>cat_11</th>\n",
       "      <th>cat_12</th>\n",
       "      <th>num_42</th>\n",
       "      <th>cat_13</th>\n",
       "      <th>num_43</th>\n",
       "      <th>num_44</th>\n",
       "      <th>num_45</th>\n",
       "      <th>cat_14</th>\n",
       "      <th>cat_15</th>\n",
       "      <th>num_46</th>\n",
       "      <th>num_47</th>\n",
       "      <th>num_48</th>\n",
       "      <th>num_49</th>\n",
       "      <th>num_50</th>\n",
       "      <th>num_51</th>\n",
       "      <th>num_52</th>\n",
       "      <th>num_53</th>\n",
       "      <th>num_54</th>\n",
       "      <th>cat_16</th>\n",
       "      <th>num_55</th>\n",
       "      <th>num_56</th>\n",
       "      <th>num_57</th>\n",
       "      <th>cat_17</th>\n",
       "      <th>num_58</th>\n",
       "      <th>cat_18</th>\n",
       "      <th>num_59</th>\n",
       "      <th>num_60</th>\n",
       "      <th>num_61</th>\n",
       "      <th>num_62</th>\n",
       "      <th>num_63</th>\n",
       "      <th>num_64</th>\n",
       "      <th>num_65</th>\n",
       "      <th>cat_19</th>\n",
       "      <th>num_66</th>\n",
       "      <th>num_67</th>\n",
       "      <th>num_68</th>\n",
       "      <th>num_69</th>\n",
       "      <th>num_70</th>\n",
       "      <th>num_71</th>\n",
       "      <th>cat_20</th>\n",
       "      <th>num_72</th>\n",
       "      <th>num_73</th>\n",
       "      <th>num_74</th>\n",
       "      <th>num_75</th>\n",
       "      <th>num_76</th>\n",
       "      <th>cat_21</th>\n",
       "      <th>num_77</th>\n",
       "      <th>num_78</th>\n",
       "      <th>cat_22</th>\n",
       "      <th>num_79</th>\n",
       "      <th>cat_23</th>\n",
       "      <th>cat_24</th>\n",
       "      <th>num_80</th>\n",
       "      <th>num_81</th>\n",
       "      <th>cat_25</th>\n",
       "      <th>num_82</th>\n",
       "      <th>cat_26</th>\n",
       "      <th>cat_27</th>\n",
       "      <th>cat_28</th>\n",
       "      <th>num_83</th>\n",
       "      <th>num_84</th>\n",
       "      <th>num_85</th>\n",
       "      <th>cat_29</th>\n",
       "      <th>num_86</th>\n",
       "      <th>num_87</th>\n",
       "      <th>num_88</th>\n",
       "      <th>num_89</th>\n",
       "      <th>num_90</th>\n",
       "      <th>cat_30</th>\n",
       "      <th>num_91</th>\n",
       "      <th>cat_31</th>\n",
       "      <th>cat_32</th>\n",
       "      <th>num_92</th>\n",
       "      <th>num_93</th>\n",
       "      <th>num_94</th>\n",
       "      <th>num_95</th>\n",
       "      <th>num_96</th>\n",
       "      <th>num_97</th>\n",
       "      <th>num_98</th>\n",
       "      <th>num_99</th>\n",
       "      <th>num_100</th>\n",
       "      <th>num_101</th>\n",
       "      <th>cat_33</th>\n",
       "      <th>num_102</th>\n",
       "      <th>num_103</th>\n",
       "      <th>num_104</th>\n",
       "      <th>num_105</th>\n",
       "      <th>num_106</th>\n",
       "      <th>num_107</th>\n",
       "      <th>num_108</th>\n",
       "      <th>num_109</th>\n",
       "      <th>num_110</th>\n",
       "      <th>num_111</th>\n",
       "      <th>num_112</th>\n",
       "      <th>cat_34</th>\n",
       "      <th>num_113</th>\n",
       "      <th>num_114</th>\n",
       "      <th>num_115</th>\n",
       "      <th>cat_35</th>\n",
       "      <th>num_116</th>\n",
       "      <th>num_117</th>\n",
       "      <th>num_118</th>\n",
       "      <th>num_119</th>\n",
       "      <th>cat_36</th>\n",
       "      <th>num_120</th>\n",
       "      <th>cat_37</th>\n",
       "      <th>num_121</th>\n",
       "      <th>num_122</th>\n",
       "      <th>cat_38</th>\n",
       "      <th>cat_39</th>\n",
       "      <th>num_123</th>\n",
       "      <th>num_124</th>\n",
       "      <th>num_125</th>\n",
       "      <th>num_126</th>\n",
       "      <th>num_127</th>\n",
       "      <th>num_128</th>\n",
       "      <th>num_129</th>\n",
       "      <th>num_130</th>\n",
       "      <th>num_131</th>\n",
       "      <th>num_132</th>\n",
       "      <th>num_133</th>\n",
       "      <th>num_134</th>\n",
       "      <th>cat_40</th>\n",
       "      <th>cat_41</th>\n",
       "      <th>cat_42</th>\n",
       "      <th>num_135</th>\n",
       "      <th>cat_43</th>\n",
       "      <th>num_136</th>\n",
       "      <th>num_137</th>\n",
       "      <th>num_138</th>\n",
       "      <th>num_139</th>\n",
       "      <th>cat_44</th>\n",
       "      <th>cat_45</th>\n",
       "      <th>cat_46</th>\n",
       "      <th>num_140</th>\n",
       "      <th>num_141</th>\n",
       "      <th>cat_47</th>\n",
       "      <th>num_142</th>\n",
       "      <th>num_143</th>\n",
       "      <th>num_144</th>\n",
       "      <th>cat_48</th>\n",
       "      <th>cat_49</th>\n",
       "      <th>cat_50</th>\n",
       "      <th>num_145</th>\n",
       "      <th>num_146</th>\n",
       "      <th>num_147</th>\n",
       "      <th>num_148</th>\n",
       "      <th>num_149</th>\n",
       "      <th>num_150</th>\n",
       "      <th>cat_51</th>\n",
       "      <th>num_151</th>\n",
       "      <th>cat_52</th>\n",
       "      <th>num_152</th>\n",
       "      <th>cat_53</th>\n",
       "      <th>num_153</th>\n",
       "      <th>num_154</th>\n",
       "      <th>num_155</th>\n",
       "      <th>num_156</th>\n",
       "      <th>num_157</th>\n",
       "      <th>cat_54</th>\n",
       "      <th>num_158</th>\n",
       "      <th>num_159</th>\n",
       "      <th>num_160</th>\n",
       "      <th>num_161</th>\n",
       "      <th>num_162</th>\n",
       "      <th>num_163</th>\n",
       "      <th>num_164</th>\n",
       "      <th>num_165</th>\n",
       "      <th>num_166</th>\n",
       "      <th>num_167</th>\n",
       "      <th>num_168</th>\n",
       "      <th>cat_55</th>\n",
       "      <th>num_169</th>\n",
       "      <th>num_170</th>\n",
       "      <th>num_171</th>\n",
       "      <th>num_172</th>\n",
       "      <th>num_173</th>\n",
       "      <th>cat_56</th>\n",
       "      <th>num_174</th>\n",
       "      <th>num_175</th>\n",
       "      <th>num_176</th>\n",
       "      <th>num_177</th>\n",
       "      <th>num_178</th>\n",
       "      <th>num_179</th>\n",
       "      <th>num_180</th>\n",
       "      <th>num_181</th>\n",
       "      <th>num_182</th>\n",
       "      <th>num_183</th>\n",
       "      <th>num_184</th>\n",
       "      <th>cat_57</th>\n",
       "      <th>num_185</th>\n",
       "      <th>cat_58</th>\n",
       "      <th>num_186</th>\n",
       "      <th>num_187</th>\n",
       "      <th>num_188</th>\n",
       "      <th>cat_59</th>\n",
       "      <th>cat_60</th>\n",
       "      <th>num_189</th>\n",
       "      <th>cat_61</th>\n",
       "      <th>num_190</th>\n",
       "      <th>num_191</th>\n",
       "      <th>num_192</th>\n",
       "      <th>num_193</th>\n",
       "      <th>num_194</th>\n",
       "      <th>num_195</th>\n",
       "      <th>num_196</th>\n",
       "      <th>num_197</th>\n",
       "      <th>num_198</th>\n",
       "      <th>cat_62</th>\n",
       "      <th>num_199</th>\n",
       "      <th>num_200</th>\n",
       "      <th>num_201</th>\n",
       "      <th>cat_63</th>\n",
       "      <th>num_202</th>\n",
       "      <th>cat_64</th>\n",
       "      <th>cat_65</th>\n",
       "      <th>cat_66</th>\n",
       "      <th>num_203</th>\n",
       "      <th>num_204</th>\n",
       "      <th>num_205</th>\n",
       "      <th>num_206</th>\n",
       "      <th>num_207</th>\n",
       "      <th>num_208</th>\n",
       "      <th>cat_67</th>\n",
       "      <th>cat_68</th>\n",
       "      <th>num_209</th>\n",
       "      <th>num_210</th>\n",
       "      <th>cat_69</th>\n",
       "      <th>cat_70</th>\n",
       "      <th>num_211</th>\n",
       "      <th>cat_71</th>\n",
       "      <th>num_212</th>\n",
       "      <th>num_213</th>\n",
       "      <th>num_214</th>\n",
       "      <th>num_215</th>\n",
       "      <th>num_216</th>\n",
       "      <th>num_217</th>\n",
       "      <th>num_218</th>\n",
       "      <th>num_219</th>\n",
       "      <th>num_220</th>\n",
       "      <th>num_221</th>\n",
       "      <th>num_222</th>\n",
       "      <th>num_223</th>\n",
       "      <th>num_224</th>\n",
       "      <th>num_225</th>\n",
       "      <th>cat_72</th>\n",
       "      <th>cat_73</th>\n",
       "      <th>num_226</th>\n",
       "      <th>num_227</th>\n",
       "      <th>num_228</th>\n",
       "      <th>cat_74</th>\n",
       "      <th>num_229</th>\n",
       "      <th>num_230</th>\n",
       "      <th>num_231</th>\n",
       "      <th>num_232</th>\n",
       "      <th>cat_75</th>\n",
       "      <th>num_233</th>\n",
       "      <th>num_234</th>\n",
       "      <th>cat_76</th>\n",
       "      <th>num_235</th>\n",
       "      <th>cat_77</th>\n",
       "      <th>num_236</th>\n",
       "      <th>num_237</th>\n",
       "      <th>num_238</th>\n",
       "      <th>cat_78</th>\n",
       "      <th>num_239</th>\n",
       "      <th>cat_79</th>\n",
       "      <th>num_240</th>\n",
       "      <th>num_241</th>\n",
       "      <th>num_242</th>\n",
       "      <th>cat_80</th>\n",
       "      <th>num_243</th>\n",
       "      <th>num_244</th>\n",
       "      <th>cat_81</th>\n",
       "      <th>cat_82</th>\n",
       "      <th>num_245</th>\n",
       "      <th>cat_83</th>\n",
       "      <th>num_246</th>\n",
       "      <th>num_247</th>\n",
       "      <th>num_248</th>\n",
       "      <th>num_249</th>\n",
       "      <th>num_250</th>\n",
       "      <th>num_251</th>\n",
       "      <th>num_252</th>\n",
       "      <th>num_253</th>\n",
       "      <th>num_254</th>\n",
       "      <th>num_255</th>\n",
       "      <th>num_256</th>\n",
       "      <th>cat_84</th>\n",
       "      <th>cat_85</th>\n",
       "      <th>num_257</th>\n",
       "      <th>cat_86</th>\n",
       "      <th>num_258</th>\n",
       "      <th>num_259</th>\n",
       "      <th>cat_87</th>\n",
       "      <th>cat_88</th>\n",
       "      <th>num_260</th>\n",
       "      <th>num_261</th>\n",
       "      <th>num_262</th>\n",
       "      <th>cat_89</th>\n",
       "      <th>num_263</th>\n",
       "      <th>cat_90</th>\n",
       "      <th>cat_91</th>\n",
       "      <th>cat_92</th>\n",
       "      <th>num_264</th>\n",
       "      <th>num_265</th>\n",
       "      <th>num_266</th>\n",
       "      <th>num_267</th>\n",
       "      <th>num_268</th>\n",
       "      <th>num_269</th>\n",
       "      <th>num_270</th>\n",
       "      <th>num_271</th>\n",
       "      <th>cat_93</th>\n",
       "      <th>num_272</th>\n",
       "      <th>num_273</th>\n",
       "      <th>num_274</th>\n",
       "      <th>cat_94</th>\n",
       "      <th>cat_95</th>\n",
       "      <th>num_275</th>\n",
       "      <th>cat_96</th>\n",
       "      <th>num_276</th>\n",
       "      <th>num_277</th>\n",
       "      <th>num_278</th>\n",
       "      <th>num_279</th>\n",
       "      <th>cat_97</th>\n",
       "      <th>num_280</th>\n",
       "      <th>num_281</th>\n",
       "      <th>num_282</th>\n",
       "      <th>num_283</th>\n",
       "      <th>num_284</th>\n",
       "      <th>num_285</th>\n",
       "      <th>num_286</th>\n",
       "      <th>num_287</th>\n",
       "      <th>num_288</th>\n",
       "      <th>num_289</th>\n",
       "      <th>num_290</th>\n",
       "      <th>num_291</th>\n",
       "      <th>num_292</th>\n",
       "      <th>cat_98</th>\n",
       "      <th>num_293</th>\n",
       "      <th>num_294</th>\n",
       "      <th>num_295</th>\n",
       "      <th>num_296</th>\n",
       "      <th>cat_99</th>\n",
       "      <th>cat_100</th>\n",
       "      <th>num_297</th>\n",
       "      <th>num_298</th>\n",
       "      <th>num_299</th>\n",
       "      <th>num_300</th>\n",
       "      <th>num_301</th>\n",
       "      <th>cat_101</th>\n",
       "      <th>cat_102</th>\n",
       "      <th>num_302</th>\n",
       "      <th>cat_103</th>\n",
       "      <th>num_303</th>\n",
       "      <th>num_304</th>\n",
       "      <th>cat_104</th>\n",
       "      <th>cat_105</th>\n",
       "      <th>num_305</th>\n",
       "      <th>num_306</th>\n",
       "      <th>num_307</th>\n",
       "      <th>num_308</th>\n",
       "      <th>num_309</th>\n",
       "      <th>num_310</th>\n",
       "      <th>num_311</th>\n",
       "      <th>num_312</th>\n",
       "      <th>num_313</th>\n",
       "      <th>num_314</th>\n",
       "      <th>cat_106</th>\n",
       "      <th>num_315</th>\n",
       "      <th>cat_107</th>\n",
       "      <th>num_316</th>\n",
       "      <th>cat_108</th>\n",
       "      <th>num_317</th>\n",
       "      <th>num_318</th>\n",
       "      <th>num_319</th>\n",
       "      <th>num_320</th>\n",
       "      <th>num_321</th>\n",
       "      <th>num_322</th>\n",
       "      <th>num_323</th>\n",
       "      <th>num_324</th>\n",
       "      <th>num_325</th>\n",
       "      <th>num_326</th>\n",
       "      <th>cat_109</th>\n",
       "      <th>cat_110</th>\n",
       "      <th>num_327</th>\n",
       "      <th>num_328</th>\n",
       "      <th>cat_111</th>\n",
       "      <th>cat_112</th>\n",
       "      <th>num_329</th>\n",
       "      <th>num_330</th>\n",
       "      <th>num_331</th>\n",
       "      <th>num_332</th>\n",
       "      <th>num_333</th>\n",
       "      <th>num_334</th>\n",
       "      <th>num_335</th>\n",
       "      <th>num_336</th>\n",
       "      <th>num_337</th>\n",
       "      <th>num_338</th>\n",
       "      <th>num_339</th>\n",
       "      <th>num_340</th>\n",
       "      <th>num_341</th>\n",
       "      <th>num_342</th>\n",
       "      <th>cat_113</th>\n",
       "      <th>num_343</th>\n",
       "      <th>num_344</th>\n",
       "      <th>num_345</th>\n",
       "      <th>cat_114</th>\n",
       "      <th>num_346</th>\n",
       "      <th>num_347</th>\n",
       "      <th>num_348</th>\n",
       "      <th>cat_115</th>\n",
       "      <th>num_349</th>\n",
       "      <th>cat_116</th>\n",
       "      <th>cat_117</th>\n",
       "      <th>num_350</th>\n",
       "      <th>cat_118</th>\n",
       "      <th>num_351</th>\n",
       "      <th>num_352</th>\n",
       "      <th>num_353</th>\n",
       "      <th>num_354</th>\n",
       "      <th>num_355</th>\n",
       "      <th>num_356</th>\n",
       "      <th>num_357</th>\n",
       "      <th>num_358</th>\n",
       "      <th>num_359</th>\n",
       "      <th>num_360</th>\n",
       "      <th>num_361</th>\n",
       "      <th>num_362</th>\n",
       "      <th>num_363</th>\n",
       "      <th>num_364</th>\n",
       "      <th>num_365</th>\n",
       "      <th>num_366</th>\n",
       "      <th>num_367</th>\n",
       "      <th>cat_119</th>\n",
       "      <th>num_368</th>\n",
       "      <th>cat_120</th>\n",
       "      <th>num_369</th>\n",
       "      <th>num_370</th>\n",
       "      <th>num_371</th>\n",
       "      <th>num_372</th>\n",
       "      <th>num_373</th>\n",
       "      <th>num_374</th>\n",
       "      <th>num_375</th>\n",
       "      <th>cat_121</th>\n",
       "      <th>num_376</th>\n",
       "      <th>num_377</th>\n",
       "      <th>num_378</th>\n",
       "      <th>num_379</th>\n",
       "      <th>num_380</th>\n",
       "      <th>num_381</th>\n",
       "      <th>num_382</th>\n",
       "      <th>num_383</th>\n",
       "      <th>num_384</th>\n",
       "      <th>num_385</th>\n",
       "      <th>num_386</th>\n",
       "      <th>cat_122</th>\n",
       "      <th>num_387</th>\n",
       "      <th>cat_123</th>\n",
       "      <th>cat_124</th>\n",
       "      <th>num_388</th>\n",
       "      <th>num_389</th>\n",
       "      <th>num_390</th>\n",
       "      <th>num_391</th>\n",
       "      <th>cat_125</th>\n",
       "      <th>num_392</th>\n",
       "      <th>num_393</th>\n",
       "      <th>cat_126</th>\n",
       "      <th>cat_127</th>\n",
       "      <th>cat_128</th>\n",
       "      <th>cat_129</th>\n",
       "      <th>num_394</th>\n",
       "      <th>num_395</th>\n",
       "      <th>num_396</th>\n",
       "      <th>num_397</th>\n",
       "      <th>cat_130</th>\n",
       "      <th>num_398</th>\n",
       "      <th>num_399</th>\n",
       "      <th>num_400</th>\n",
       "      <th>num_401</th>\n",
       "      <th>cat_131</th>\n",
       "      <th>num_402</th>\n",
       "      <th>num_403</th>\n",
       "      <th>num_404</th>\n",
       "      <th>num_405</th>\n",
       "      <th>num_406</th>\n",
       "      <th>num_407</th>\n",
       "      <th>num_408</th>\n",
       "      <th>num_409</th>\n",
       "      <th>num_410</th>\n",
       "      <th>num_411</th>\n",
       "      <th>num_412</th>\n",
       "      <th>num_413</th>\n",
       "      <th>cat_132</th>\n",
       "      <th>cat_133</th>\n",
       "      <th>num_414</th>\n",
       "      <th>num_415</th>\n",
       "      <th>num_416</th>\n",
       "      <th>cat_134</th>\n",
       "      <th>cat_135</th>\n",
       "      <th>id</th>\n",
       "      <th>gb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1377.3</td>\n",
       "      <td>3712.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1303.0</td>\n",
       "      <td>2409.0</td>\n",
       "      <td>1281.7</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>195.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>417.1</td>\n",
       "      <td>2210.0</td>\n",
       "      <td>12970.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>343.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11327.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "      <td>-10446.6</td>\n",
       "      <td>1</td>\n",
       "      <td>559.600</td>\n",
       "      <td>18598389.68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>74742.52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7768369.20</td>\n",
       "      <td>559.600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1142.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>498892.16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2446146.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11306764.14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1371743.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-1027763.45</td>\n",
       "      <td>2102.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.03</td>\n",
       "      <td>1</td>\n",
       "      <td>795055.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.08</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26672630.02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>458.400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7933.3</td>\n",
       "      <td>7933.3</td>\n",
       "      <td>6675.8</td>\n",
       "      <td>9506.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.8</td>\n",
       "      <td>1307.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>-99.80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11728.9</td>\n",
       "      <td>11589.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7933.3</td>\n",
       "      <td>195.0</td>\n",
       "      <td>-25.3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3394.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>46913.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2898.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.3</td>\n",
       "      <td>-440123.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28333733.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3303.2</td>\n",
       "      <td>3303.2</td>\n",
       "      <td>28.68</td>\n",
       "      <td>1</td>\n",
       "      <td>11327.5</td>\n",
       "      <td>1820.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7933.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2544058.39</td>\n",
       "      <td>1</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11728.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.517000e-01</td>\n",
       "      <td>2270.0</td>\n",
       "      <td>39184.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>224.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6728.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1524.3</td>\n",
       "      <td>3565.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5034.4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2830.8</td>\n",
       "      <td>-1112.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3836906.0</td>\n",
       "      <td>6774.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>27276369.89</td>\n",
       "      <td>0.65</td>\n",
       "      <td>50.0</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>377973.06</td>\n",
       "      <td>923.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2270.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.356800e+00</td>\n",
       "      <td>7.822200e+00</td>\n",
       "      <td>0.7003</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.021000e+00</td>\n",
       "      <td>2.877900e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14938.7</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.93</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1438784.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3303.2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>616000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4694287.90</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10214.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>23.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31425.20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>297.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28344782.47</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>90.65</td>\n",
       "      <td>2274.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2270.0</td>\n",
       "      <td>1</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1967.9</td>\n",
       "      <td>1033242.0</td>\n",
       "      <td>1</td>\n",
       "      <td>14938.7</td>\n",
       "      <td>1</td>\n",
       "      <td>22123.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>91.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1014.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>443.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-139.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>324.57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>46913.20</td>\n",
       "      <td>9506.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11327.5</td>\n",
       "      <td>41786.400</td>\n",
       "      <td>41786.400</td>\n",
       "      <td>46913.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2163.0</td>\n",
       "      <td>1</td>\n",
       "      <td>413050.73</td>\n",
       "      <td>6.0</td>\n",
       "      <td>443.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2260.3</td>\n",
       "      <td>73.9644</td>\n",
       "      <td>29.2400</td>\n",
       "      <td>24.7267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.648000e-01</td>\n",
       "      <td>0.6157</td>\n",
       "      <td>6515.1</td>\n",
       "      <td>4.9348</td>\n",
       "      <td>91.0</td>\n",
       "      <td>4830.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.7434</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7783775.80</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1820.9</td>\n",
       "      <td>229.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>139.9</td>\n",
       "      <td>5126.800</td>\n",
       "      <td>286.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11327.5</td>\n",
       "      <td>286.9</td>\n",
       "      <td>1014.2</td>\n",
       "      <td>559.600</td>\n",
       "      <td>1</td>\n",
       "      <td>60.1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>50.00</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5034.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11327.5</td>\n",
       "      <td>3303.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1014.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6732.0</td>\n",
       "      <td>2088171.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7933.3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7818.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65084.25</td>\n",
       "      <td>1</td>\n",
       "      <td>9506.6</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.869000e+08</td>\n",
       "      <td>211.1</td>\n",
       "      <td>1.399000e+08</td>\n",
       "      <td>5.055000e+00</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>4.940000e-02</td>\n",
       "      <td>5034375.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>86.06</td>\n",
       "      <td>11728.3</td>\n",
       "      <td>28.68</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139.9</td>\n",
       "      <td>11327.5</td>\n",
       "      <td>46913.20</td>\n",
       "      <td>559.600</td>\n",
       "      <td>6974.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>825009.5</td>\n",
       "      <td>3469909.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2784.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4743.92</td>\n",
       "      <td>3758.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>9.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7029.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>500000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11728.9</td>\n",
       "      <td>11728.3</td>\n",
       "      <td>1014.2</td>\n",
       "      <td>3454.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1014.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46913.20</td>\n",
       "      <td>11327.5</td>\n",
       "      <td>11327.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28162496.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>565.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>357.2</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>170.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.0</td>\n",
       "      <td>123.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-986.3</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000</td>\n",
       "      <td>963054.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41999.36</td>\n",
       "      <td>25.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-332.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14982.75</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>77309.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>422913.14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54093.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-14982.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.70</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.17</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>974642.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>168.6</td>\n",
       "      <td>168.6</td>\n",
       "      <td>148.6</td>\n",
       "      <td>148.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>569.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1343.5</td>\n",
       "      <td>1318.5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>168.6</td>\n",
       "      <td>170.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39927.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1343.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2636.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>921779.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.33</td>\n",
       "      <td>1</td>\n",
       "      <td>168.6</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158062.81</td>\n",
       "      <td>1</td>\n",
       "      <td>35000.00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1343.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.500000e+07</td>\n",
       "      <td>967.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>967.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>20.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-399.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142472.0</td>\n",
       "      <td>121.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>279426.28</td>\n",
       "      <td>0.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35000.00</td>\n",
       "      <td>10.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1410.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>967.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.430000e+00</td>\n",
       "      <td>1.686000e+08</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>400000.0000</td>\n",
       "      <td>1.486000e+08</td>\n",
       "      <td>1.486000e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1482</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>143.6</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7405.74</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14982.75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2658</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>163857.03</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7058.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1601.78</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>988142.21</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34.72</td>\n",
       "      <td>971.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>955.0</td>\n",
       "      <td>1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>143.6</td>\n",
       "      <td>1</td>\n",
       "      <td>39927.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1343.50</td>\n",
       "      <td>148.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.6</td>\n",
       "      <td>986.300</td>\n",
       "      <td>986.300</td>\n",
       "      <td>1343.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6390.78</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.3714</td>\n",
       "      <td>40.2627</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.500000e+07</td>\n",
       "      <td>0.8790</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29782.5</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-41999.36</td>\n",
       "      <td>124.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>357.200</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.6</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.000</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>53.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>971.0</td>\n",
       "      <td>157527.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.6</td>\n",
       "      <td>38.0</td>\n",
       "      <td>2</td>\n",
       "      <td>143.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>148.6</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.500000e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.500000e+07</td>\n",
       "      <td>3.572000e+08</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.482000e-01</td>\n",
       "      <td>200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.01</td>\n",
       "      <td>1343.5</td>\n",
       "      <td>50.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>168.6</td>\n",
       "      <td>1343.50</td>\n",
       "      <td>25.000</td>\n",
       "      <td>148.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54239.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>143.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6419.05</td>\n",
       "      <td>148.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>35000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1343.5</td>\n",
       "      <td>1343.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1343.50</td>\n",
       "      <td>168.6</td>\n",
       "      <td>168.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>989383.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.56</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>150.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2294.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>107.6</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "      <td>196.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.5</td>\n",
       "      <td>33.0</td>\n",
       "      <td>71.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>209.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-161.3</td>\n",
       "      <td>1</td>\n",
       "      <td>99.800</td>\n",
       "      <td>71843.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112589.52</td>\n",
       "      <td>20.200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.0</td>\n",
       "      <td>30734.10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>174358.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81476.86</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-19474.60</td>\n",
       "      <td>20.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.39</td>\n",
       "      <td>1</td>\n",
       "      <td>71127.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.02</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>140.8</td>\n",
       "      <td>140.8</td>\n",
       "      <td>48.9</td>\n",
       "      <td>58.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2298.0</td>\n",
       "      <td>-79.6</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>-100.38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>268.9</td>\n",
       "      <td>248.7</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>140.8</td>\n",
       "      <td>196.0</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.3</td>\n",
       "      <td>28656.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>268.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.4</td>\n",
       "      <td>-13634.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>176490.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.3</td>\n",
       "      <td>68.3</td>\n",
       "      <td>40.56</td>\n",
       "      <td>1</td>\n",
       "      <td>209.1</td>\n",
       "      <td>150.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>140.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5183.07</td>\n",
       "      <td>1</td>\n",
       "      <td>31000.00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>268.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.496200e+00</td>\n",
       "      <td>2297.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3167.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>150.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121.6</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.6</td>\n",
       "      <td>-2102.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>18440.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>150412.34</td>\n",
       "      <td>0.40</td>\n",
       "      <td>46.0</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>3493.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12156.37</td>\n",
       "      <td>13.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2297.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.349000e-01</td>\n",
       "      <td>1.042960e+01</td>\n",
       "      <td>0.6733</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4772</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.159000e-01</td>\n",
       "      <td>8.565000e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>222.2</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.84</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>32.5</td>\n",
       "      <td>18260.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.3</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>29390.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>495.39</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>41.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>176340.03</td>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.82</td>\n",
       "      <td>2301.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2297.0</td>\n",
       "      <td>1</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.6</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>1</td>\n",
       "      <td>222.2</td>\n",
       "      <td>1</td>\n",
       "      <td>43897.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-93.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5402.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>268.90</td>\n",
       "      <td>58.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>209.1</td>\n",
       "      <td>161.300</td>\n",
       "      <td>161.300</td>\n",
       "      <td>268.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>13412.18</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.3</td>\n",
       "      <td>79.4068</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>21.7234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.957000e-01</td>\n",
       "      <td>0.1731</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.5965</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.7817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-112154.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>150.6</td>\n",
       "      <td>1289.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99.8</td>\n",
       "      <td>107.600</td>\n",
       "      <td>99.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>209.1</td>\n",
       "      <td>99.8</td>\n",
       "      <td>13.5</td>\n",
       "      <td>20.200</td>\n",
       "      <td>1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>41.50</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>121.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>209.1</td>\n",
       "      <td>68.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3171.0</td>\n",
       "      <td>2468.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.8</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>124.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>58.5</td>\n",
       "      <td>0.3711</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.253700e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.253700e+00</td>\n",
       "      <td>7.970300e+00</td>\n",
       "      <td>0.0645</td>\n",
       "      <td>9.660000e-02</td>\n",
       "      <td>121622.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.39</td>\n",
       "      <td>268.9</td>\n",
       "      <td>40.56</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.2</td>\n",
       "      <td>209.1</td>\n",
       "      <td>268.90</td>\n",
       "      <td>20.200</td>\n",
       "      <td>36.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3029.5</td>\n",
       "      <td>21352.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>72.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>268.9</td>\n",
       "      <td>268.9</td>\n",
       "      <td>13.5</td>\n",
       "      <td>10.2</td>\n",
       "      <td>79.6</td>\n",
       "      <td>13.5</td>\n",
       "      <td>79.6</td>\n",
       "      <td>268.90</td>\n",
       "      <td>209.1</td>\n",
       "      <td>209.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>87444.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>11.4</td>\n",
       "      <td>178.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1236.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350.6</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>117.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>25.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>57.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.0</td>\n",
       "      <td>-755.9</td>\n",
       "      <td>1</td>\n",
       "      <td>143.100</td>\n",
       "      <td>237787.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12078.50</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48983.65</td>\n",
       "      <td>143.100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-208.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23600.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>68230.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>119534.17</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6935.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.18</td>\n",
       "      <td>-21085.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.64</td>\n",
       "      <td>1</td>\n",
       "      <td>169.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.35</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>277608.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117.400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-40.7</td>\n",
       "      <td>-40.7</td>\n",
       "      <td>22.3</td>\n",
       "      <td>45.5</td>\n",
       "      <td>1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.5</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>-112.12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1107.8</td>\n",
       "      <td>964.7</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-40.7</td>\n",
       "      <td>117.0</td>\n",
       "      <td>-25.7</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.6</td>\n",
       "      <td>4773.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1106.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-54.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>23600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.4</td>\n",
       "      <td>-7901.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>307216.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.6</td>\n",
       "      <td>97.6</td>\n",
       "      <td>57.45</td>\n",
       "      <td>1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>11.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-40.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>109335.32</td>\n",
       "      <td>1</td>\n",
       "      <td>23600.00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1106.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.431000e+08</td>\n",
       "      <td>1244.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1247.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>12.8</td>\n",
       "      <td>176.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.8</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.2</td>\n",
       "      <td>-1123.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.02</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42092.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>165541.67</td>\n",
       "      <td>0.79</td>\n",
       "      <td>49.0</td>\n",
       "      <td>20000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2224.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1244.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.570100e+00</td>\n",
       "      <td>-4.070000e+07</td>\n",
       "      <td>-0.7152</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.5149</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>2.284000e-01</td>\n",
       "      <td>4.661000e-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.8845</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>70.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.5</td>\n",
       "      <td>18095.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.6</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>49735.46</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1310.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14037.20</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>298412.80</td>\n",
       "      <td>1</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.05</td>\n",
       "      <td>1248.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1244.0</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.8</td>\n",
       "      <td>8468.0</td>\n",
       "      <td>1</td>\n",
       "      <td>70.4</td>\n",
       "      <td>1</td>\n",
       "      <td>4773.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117.4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-85.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15205.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1106.50</td>\n",
       "      <td>45.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.9</td>\n",
       "      <td>755.900</td>\n",
       "      <td>755.900</td>\n",
       "      <td>1106.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>30.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5589.90</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.2</td>\n",
       "      <td>15.0090</td>\n",
       "      <td>6.2675</td>\n",
       "      <td>11.2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.466100e+00</td>\n",
       "      <td>-0.8594</td>\n",
       "      <td>133.8</td>\n",
       "      <td>24.3186</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.3770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-43685.58</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>11.4</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>143.1</td>\n",
       "      <td>350.600</td>\n",
       "      <td>144.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>143.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.9</td>\n",
       "      <td>144.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>143.100</td>\n",
       "      <td>1</td>\n",
       "      <td>10.9</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>54.33</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.9</td>\n",
       "      <td>97.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1251.0</td>\n",
       "      <td>13654.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-40.7</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-158.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16422.00</td>\n",
       "      <td>1</td>\n",
       "      <td>45.5</td>\n",
       "      <td>0.1305</td>\n",
       "      <td>0.1291</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.445000e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.431000e+08</td>\n",
       "      <td>3.506000e+08</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.514900e+00</td>\n",
       "      <td>13800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1106.5</td>\n",
       "      <td>57.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>143.1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>1106.50</td>\n",
       "      <td>143.100</td>\n",
       "      <td>-48.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15403.0</td>\n",
       "      <td>16607.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-171.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5859.27</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>39.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>228.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1107.8</td>\n",
       "      <td>1106.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1106.50</td>\n",
       "      <td>56.9</td>\n",
       "      <td>56.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>297608.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>5372.0</td>\n",
       "      <td>3386.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>716.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>401.0</td>\n",
       "      <td>2447.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1282.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60925.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1335.0</td>\n",
       "      <td>1</td>\n",
       "      <td>28.000</td>\n",
       "      <td>1665741.11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>128520.23</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>779486.22</td>\n",
       "      <td>28.000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53918.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>53807.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>174363.15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>461110.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1292247.39</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120815.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-95418.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.14</td>\n",
       "      <td>1</td>\n",
       "      <td>3701982.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2192372.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59541.0</td>\n",
       "      <td>59541.0</td>\n",
       "      <td>1073.0</td>\n",
       "      <td>1473.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189.0</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-100.19</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>59541.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5396.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>202.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>277.0</td>\n",
       "      <td>-11802.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7671415.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>49.64</td>\n",
       "      <td>1</td>\n",
       "      <td>60925.0</td>\n",
       "      <td>59452.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1349.0</td>\n",
       "      <td>59541.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>5064200.58</td>\n",
       "      <td>1</td>\n",
       "      <td>190000.00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1349.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>435.0</td>\n",
       "      <td>9.032000e-01</td>\n",
       "      <td>2203.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4408.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5111.0</td>\n",
       "      <td>3377.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4661.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.8849</td>\n",
       "      <td>400.0</td>\n",
       "      <td>-1335.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.34</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>500.0</td>\n",
       "      <td>114630.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2469064.87</td>\n",
       "      <td>0.24</td>\n",
       "      <td>54.0</td>\n",
       "      <td>35000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.2907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27896.32</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2203.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.001400e+00</td>\n",
       "      <td>1.920677e+03</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.752000e-01</td>\n",
       "      <td>1.064300e+00</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>61800.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73359.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>890391.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8634.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>76.0</td>\n",
       "      <td>742.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18251.11</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>303.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6733855.70</td>\n",
       "      <td>1</td>\n",
       "      <td>195.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>53.36</td>\n",
       "      <td>2207.0</td>\n",
       "      <td>1</td>\n",
       "      <td>52.13</td>\n",
       "      <td>2203.0</td>\n",
       "      <td>1</td>\n",
       "      <td>73.0</td>\n",
       "      <td>5</td>\n",
       "      <td>53807.0</td>\n",
       "      <td>59353.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>61800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4861.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>435.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-76.88</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2052.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5396.00</td>\n",
       "      <td>1473.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60925.0</td>\n",
       "      <td>5340.000</td>\n",
       "      <td>5340.000</td>\n",
       "      <td>5396.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60925.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>249.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2052.10</td>\n",
       "      <td>11.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252.0</td>\n",
       "      <td>99.6376</td>\n",
       "      <td>22.3897</td>\n",
       "      <td>27.3408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.910000e-02</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>906.0</td>\n",
       "      <td>3.6632</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-777940.94</td>\n",
       "      <td>59.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53918.0</td>\n",
       "      <td>1</td>\n",
       "      <td>59452.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>56.000</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60925.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>68.000</td>\n",
       "      <td>1</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>43.33</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5421.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60925.0</td>\n",
       "      <td>1384.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4412.0</td>\n",
       "      <td>496957.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59541.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1</td>\n",
       "      <td>59151.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>152689.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1473.0</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.700000e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.000000e+06</td>\n",
       "      <td>1.806400e+00</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2.193500e+00</td>\n",
       "      <td>5420654.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>64.80</td>\n",
       "      <td>1349.0</td>\n",
       "      <td>49.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53918.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>60925.0</td>\n",
       "      <td>5396.00</td>\n",
       "      <td>68.000</td>\n",
       "      <td>120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91980.0</td>\n",
       "      <td>537645.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>196.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3161.35</td>\n",
       "      <td>331.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2649.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>190000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>1349.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>435.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5396.00</td>\n",
       "      <td>60925.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6614247.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26819</th>\n",
       "      <td>61104</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-186.4</td>\n",
       "      <td>2</td>\n",
       "      <td>96.425</td>\n",
       "      <td>698800.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17982.12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>49.0</td>\n",
       "      <td>36157.12</td>\n",
       "      <td>113.715</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-13.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>40000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>342375.59</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-7871.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.60</td>\n",
       "      <td>1</td>\n",
       "      <td>100000.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.16</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>738801.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>93.233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>170.1</td>\n",
       "      <td>170.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>181.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>321.0</td>\n",
       "      <td>235.5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>170.1</td>\n",
       "      <td>123.0</td>\n",
       "      <td>-15.4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.3</td>\n",
       "      <td>63477.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>361.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-281.13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>975317.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.3</td>\n",
       "      <td>11.3</td>\n",
       "      <td>99.39</td>\n",
       "      <td>2</td>\n",
       "      <td>181.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159050.04</td>\n",
       "      <td>3</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.137150e+08</td>\n",
       "      <td>268.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181.3</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>747785.45</td>\n",
       "      <td>0.72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>710.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>268.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.701000e+08</td>\n",
       "      <td>1.701000e+08</td>\n",
       "      <td>0.9377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5315</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>8.800000e-03</td>\n",
       "      <td>1.605300e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1313</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7871.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>161316.83</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43095.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9813.49</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>967901.00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82.78</td>\n",
       "      <td>272.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>119.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66174.5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>63477.91</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>168.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>361.76</td>\n",
       "      <td>181.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181.4</td>\n",
       "      <td>247.912</td>\n",
       "      <td>247.912</td>\n",
       "      <td>361.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7871.78</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>183.0246</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>266.9273</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.006320e+01</td>\n",
       "      <td>0.9371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.9942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-36157.12</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>72.5</td>\n",
       "      <td>113.848</td>\n",
       "      <td>72.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181.4</td>\n",
       "      <td>72.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113.715</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>43.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>181.4</td>\n",
       "      <td>11.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>272.0</td>\n",
       "      <td>69880.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170.1</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11988.08</td>\n",
       "      <td>1</td>\n",
       "      <td>181.4</td>\n",
       "      <td>0.2665</td>\n",
       "      <td>0.2663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.013800e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-5.576900e+00</td>\n",
       "      <td>1.138480e+08</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.268000e-01</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.31</td>\n",
       "      <td>272.0</td>\n",
       "      <td>99.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.5</td>\n",
       "      <td>181.4</td>\n",
       "      <td>361.76</td>\n",
       "      <td>113.715</td>\n",
       "      <td>170.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32993.16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>321.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>361.76</td>\n",
       "      <td>181.4</td>\n",
       "      <td>181.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>840801.82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5242</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26820</th>\n",
       "      <td>61105</td>\n",
       "      <td>4.2</td>\n",
       "      <td>97.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2149.0</td>\n",
       "      <td>130.5</td>\n",
       "      <td>409.3</td>\n",
       "      <td>72.0</td>\n",
       "      <td>1</td>\n",
       "      <td>157.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.0</td>\n",
       "      <td>775.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>596.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-340.7</td>\n",
       "      <td>1</td>\n",
       "      <td>-199.101</td>\n",
       "      <td>224719.74</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14212.11</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>91.4</td>\n",
       "      <td>24076.08</td>\n",
       "      <td>-77.539</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-559.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>28.0</td>\n",
       "      <td>50903.89</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>13865.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150783.01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34821.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>163.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.96</td>\n",
       "      <td>1</td>\n",
       "      <td>250873.13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>276854.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-77.539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>543.8</td>\n",
       "      <td>543.8</td>\n",
       "      <td>489.2</td>\n",
       "      <td>571.2</td>\n",
       "      <td>1</td>\n",
       "      <td>205.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2153.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-151.52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>841.4</td>\n",
       "      <td>899.7</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>543.8</td>\n",
       "      <td>157.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.9</td>\n",
       "      <td>101791.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>997.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>543.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-292.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>572621.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.9</td>\n",
       "      <td>52.9</td>\n",
       "      <td>57.85</td>\n",
       "      <td>2</td>\n",
       "      <td>596.7</td>\n",
       "      <td>25.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>543.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>347079.90</td>\n",
       "      <td>2</td>\n",
       "      <td>62500.00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>750.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7.753900e+07</td>\n",
       "      <td>2514.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3886.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4.5</td>\n",
       "      <td>96.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.0</td>\n",
       "      <td>-1996.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>36676.0</td>\n",
       "      <td>258.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>144860.97</td>\n",
       "      <td>0.40</td>\n",
       "      <td>55.0</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5981</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>350.52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2514.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.132540e+01</td>\n",
       "      <td>5.438000e+08</td>\n",
       "      <td>0.9113</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.3336</td>\n",
       "      <td>3.8790</td>\n",
       "      <td>9.247600e+00</td>\n",
       "      <td>1.079770e+01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.1425</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>801.2</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>915</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.4</td>\n",
       "      <td>8187.35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>75521.34</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4703.38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1983.28</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>557728.07</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.36</td>\n",
       "      <td>2518.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2514.0</td>\n",
       "      <td>1</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2</td>\n",
       "      <td>21.3</td>\n",
       "      <td>25.8</td>\n",
       "      <td>29930.0</td>\n",
       "      <td>1</td>\n",
       "      <td>801.2</td>\n",
       "      <td>1</td>\n",
       "      <td>121256.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-58.3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1122.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>997.50</td>\n",
       "      <td>571.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>596.7</td>\n",
       "      <td>453.131</td>\n",
       "      <td>453.131</td>\n",
       "      <td>997.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>48.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3333.07</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>209.0105</td>\n",
       "      <td>34.9082</td>\n",
       "      <td>66.0515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.465700e+00</td>\n",
       "      <td>0.5572</td>\n",
       "      <td>49.2</td>\n",
       "      <td>1.7463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.3321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-15888.73</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.3</td>\n",
       "      <td>1</td>\n",
       "      <td>25.5</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-149.7</td>\n",
       "      <td>544.369</td>\n",
       "      <td>-149.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-58.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>596.7</td>\n",
       "      <td>-149.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-77.539</td>\n",
       "      <td>2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>41.50</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>596.7</td>\n",
       "      <td>52.9</td>\n",
       "      <td>268.2</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>205.2</td>\n",
       "      <td>3890.0</td>\n",
       "      <td>37770.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>543.8</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>602.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7964.51</td>\n",
       "      <td>1</td>\n",
       "      <td>571.2</td>\n",
       "      <td>-0.1992</td>\n",
       "      <td>-0.0692</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.494000e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.637800e+00</td>\n",
       "      <td>5.443690e+08</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-1.299000e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.38</td>\n",
       "      <td>750.0</td>\n",
       "      <td>57.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-58.3</td>\n",
       "      <td>596.7</td>\n",
       "      <td>997.50</td>\n",
       "      <td>-77.539</td>\n",
       "      <td>332.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8760.0</td>\n",
       "      <td>208488.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>601.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2291.57</td>\n",
       "      <td>95.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>81.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>199.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>15000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>841.4</td>\n",
       "      <td>750.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.1</td>\n",
       "      <td>-91.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>997.50</td>\n",
       "      <td>596.7</td>\n",
       "      <td>596.7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>557554.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26821</th>\n",
       "      <td>61108</td>\n",
       "      <td>4368.0</td>\n",
       "      <td>5004.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2215.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>1</td>\n",
       "      <td>381.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1041.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>223.0</td>\n",
       "      <td>11131.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>940.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14599.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>112.0</td>\n",
       "      <td>23954.0</td>\n",
       "      <td>2</td>\n",
       "      <td>609.140</td>\n",
       "      <td>1588435.74</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21098.17</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1350208.71</td>\n",
       "      <td>921.690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8330.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>601.0</td>\n",
       "      <td>8187.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>479640.54</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1431479.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17137955.34</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>578525.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-128258.56</td>\n",
       "      <td>663.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.37</td>\n",
       "      <td>1</td>\n",
       "      <td>9258412.62</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17009553.20</td>\n",
       "      <td>693.0</td>\n",
       "      <td>755.440</td>\n",
       "      <td>458.0</td>\n",
       "      <td>89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8528.0</td>\n",
       "      <td>8528.0</td>\n",
       "      <td>6172.0</td>\n",
       "      <td>10227.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3949.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>2219.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>-127.57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8528.0</td>\n",
       "      <td>381.0</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6071.0</td>\n",
       "      <td>106477.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36082.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>535.0</td>\n",
       "      <td>-19617.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28391614.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6071.0</td>\n",
       "      <td>6071.0</td>\n",
       "      <td>27.96</td>\n",
       "      <td>2</td>\n",
       "      <td>14599.0</td>\n",
       "      <td>4372.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>27130.0</td>\n",
       "      <td>8528.0</td>\n",
       "      <td>2642.0</td>\n",
       "      <td>5869403.31</td>\n",
       "      <td>2</td>\n",
       "      <td>489783.82</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.216900e+08</td>\n",
       "      <td>2515.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>235.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5663.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>4554.0</td>\n",
       "      <td>4438.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>4012.0</td>\n",
       "      <td>-1838.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.31</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3507947.0</td>\n",
       "      <td>1922.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>15455195.30</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>489783.82</td>\n",
       "      <td>27.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37228.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2515.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.950500e+00</td>\n",
       "      <td>8.528000e+09</td>\n",
       "      <td>0.5841</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.6504</td>\n",
       "      <td>1.016600e+00</td>\n",
       "      <td>1.684500e+00</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0885</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15793.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1959.67</td>\n",
       "      <td>2</td>\n",
       "      <td>914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>648.0</td>\n",
       "      <td>420096.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6071.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>344500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>3081589.34</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>101808.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3866.24</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27824618.73</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.98</td>\n",
       "      <td>2519.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2515.0</td>\n",
       "      <td>1</td>\n",
       "      <td>84.0</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4662.0</td>\n",
       "      <td>1480075.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15793.0</td>\n",
       "      <td>1</td>\n",
       "      <td>144088.47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>568.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>739.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-327.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>173.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>160.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36082.90</td>\n",
       "      <td>10227.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14599.0</td>\n",
       "      <td>31858.820</td>\n",
       "      <td>31858.820</td>\n",
       "      <td>36082.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>14599.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>213.0</td>\n",
       "      <td>1</td>\n",
       "      <td>291837.80</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3707.0</td>\n",
       "      <td>103.4521</td>\n",
       "      <td>16.0332</td>\n",
       "      <td>46.4573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.137000e-01</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>3395.0</td>\n",
       "      <td>3.5281</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.4703</td>\n",
       "      <td>415.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>-1058370.91</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4372.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>458.0</td>\n",
       "      <td>4224.080</td>\n",
       "      <td>1197.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1432.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14599.0</td>\n",
       "      <td>1197.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1904.560</td>\n",
       "      <td>1</td>\n",
       "      <td>1809.0</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>45.80</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>120.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14599.0</td>\n",
       "      <td>6071.0</td>\n",
       "      <td>5372.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3949.0</td>\n",
       "      <td>5667.0</td>\n",
       "      <td>5727334.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8528.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2</td>\n",
       "      <td>8317.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21098.17</td>\n",
       "      <td>1</td>\n",
       "      <td>10227.0</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.197000e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.948900e+00</td>\n",
       "      <td>4.224080e+09</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.904560e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>489783.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.14</td>\n",
       "      <td>27130.0</td>\n",
       "      <td>27.96</td>\n",
       "      <td>1828724.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>478.0</td>\n",
       "      <td>693.0</td>\n",
       "      <td>14599.0</td>\n",
       "      <td>36082.90</td>\n",
       "      <td>1904.560</td>\n",
       "      <td>855.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1353055.0</td>\n",
       "      <td>3732855.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>568.0</td>\n",
       "      <td>151475.43</td>\n",
       "      <td>1585.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7476.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>489783.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28070.0</td>\n",
       "      <td>27130.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3115.0</td>\n",
       "      <td>-235.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36082.90</td>\n",
       "      <td>14599.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>17821295.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26822</th>\n",
       "      <td>61110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>151.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1823.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1494.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-14274.2</td>\n",
       "      <td>1</td>\n",
       "      <td>16.800</td>\n",
       "      <td>6831275.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3083510.36</td>\n",
       "      <td>16.800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-8.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>199633.40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1180824.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3650601.42</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>536039.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-412981.67</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.14</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6823940.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.6</td>\n",
       "      <td>23.6</td>\n",
       "      <td>1494.9</td>\n",
       "      <td>1494.9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.2</td>\n",
       "      <td>139.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>-106.56</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>14299.9</td>\n",
       "      <td>14283.1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23.6</td>\n",
       "      <td>151.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1471.3</td>\n",
       "      <td>15206.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14299.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.5</td>\n",
       "      <td>-178269.65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7441945.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1471.3</td>\n",
       "      <td>1471.3</td>\n",
       "      <td>6.99</td>\n",
       "      <td>2</td>\n",
       "      <td>1494.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13105.43</td>\n",
       "      <td>2</td>\n",
       "      <td>200000.00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14299.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.560000e-02</td>\n",
       "      <td>740.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>741.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>225754.0</td>\n",
       "      <td>1504.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>7340792.24</td>\n",
       "      <td>0.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>118279.54</td>\n",
       "      <td>196.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>740.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.360000e+07</td>\n",
       "      <td>1.202000e-01</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.016000e+00</td>\n",
       "      <td>1.016000e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5847</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1823.3</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>423</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>602862.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1471.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1207212.60</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14250.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>177213.76</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7243275.60</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.61</td>\n",
       "      <td>744.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>147.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1823.3</td>\n",
       "      <td>1</td>\n",
       "      <td>15206.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>196.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-145.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>303.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14299.90</td>\n",
       "      <td>1494.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1494.9</td>\n",
       "      <td>14274.200</td>\n",
       "      <td>14274.200</td>\n",
       "      <td>14299.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>7.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1</td>\n",
       "      <td>195532.89</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1239.6</td>\n",
       "      <td>38.1568</td>\n",
       "      <td>37.4855</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.140000e-02</td>\n",
       "      <td>0.1470</td>\n",
       "      <td>1781.2</td>\n",
       "      <td>9.5657</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.6973</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2893629.69</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.8</td>\n",
       "      <td>25.700</td>\n",
       "      <td>16.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1494.9</td>\n",
       "      <td>16.8</td>\n",
       "      <td>196.2</td>\n",
       "      <td>16.800</td>\n",
       "      <td>1</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>45.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1494.9</td>\n",
       "      <td>1471.3</td>\n",
       "      <td>311.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>196.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>745.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.6</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1494.9</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.680000e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.680000e+07</td>\n",
       "      <td>1.309000e-01</td>\n",
       "      <td>0.1312</td>\n",
       "      <td>1.120000e-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.62</td>\n",
       "      <td>14299.9</td>\n",
       "      <td>6.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.8</td>\n",
       "      <td>1494.9</td>\n",
       "      <td>14299.90</td>\n",
       "      <td>16.800</td>\n",
       "      <td>219.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>452454.0</td>\n",
       "      <td>545638.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22353.44</td>\n",
       "      <td>1468.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1813.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>200000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14299.9</td>\n",
       "      <td>14299.9</td>\n",
       "      <td>196.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>196.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14299.90</td>\n",
       "      <td>1494.9</td>\n",
       "      <td>1494.9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7228675.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4610</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26823</th>\n",
       "      <td>61111</td>\n",
       "      <td>28.5</td>\n",
       "      <td>26.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>166.7</td>\n",
       "      <td>1706.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>104.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>75.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.0</td>\n",
       "      <td>1049.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>652.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>-9076.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1706.600</td>\n",
       "      <td>4832346.92</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19759.78</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>184601.81</td>\n",
       "      <td>198.300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>144474.70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>654455.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3559405.37</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-144474.70</td>\n",
       "      <td>144.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.29</td>\n",
       "      <td>1</td>\n",
       "      <td>1310898.90</td>\n",
       "      <td>2</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3832423.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>162.600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>457.4</td>\n",
       "      <td>457.4</td>\n",
       "      <td>348.5</td>\n",
       "      <td>624.2</td>\n",
       "      <td>1</td>\n",
       "      <td>181.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>48.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>-100.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10831.5</td>\n",
       "      <td>10633.2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>457.4</td>\n",
       "      <td>104.0</td>\n",
       "      <td>-35.7</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195.3</td>\n",
       "      <td>162861.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10783.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>257.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.1</td>\n",
       "      <td>-55529.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8664194.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195.3</td>\n",
       "      <td>195.3</td>\n",
       "      <td>56.06</td>\n",
       "      <td>2</td>\n",
       "      <td>652.7</td>\n",
       "      <td>28.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>457.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1742451.40</td>\n",
       "      <td>2</td>\n",
       "      <td>170000.00</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10783.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.983000e+08</td>\n",
       "      <td>603.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>22.9</td>\n",
       "      <td>9.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>200.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>275.7</td>\n",
       "      <td>56.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>278838.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3188437.05</td>\n",
       "      <td>0.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>21710.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>603.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.604910e+01</td>\n",
       "      <td>4.574000e+08</td>\n",
       "      <td>0.7007</td>\n",
       "      <td>0.7</td>\n",
       "      <td>2.6146</td>\n",
       "      <td>0.9303</td>\n",
       "      <td>1.784400e+00</td>\n",
       "      <td>3.196100e+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3554</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1072.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.6</td>\n",
       "      <td>16808.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1582</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1019637.13</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43220.74</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>141797.66</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8742870.92</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.04</td>\n",
       "      <td>607.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.70</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.9</td>\n",
       "      <td>100630.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1072.4</td>\n",
       "      <td>1</td>\n",
       "      <td>162861.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>162.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-11.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10783.20</td>\n",
       "      <td>624.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>652.7</td>\n",
       "      <td>9076.600</td>\n",
       "      <td>9076.600</td>\n",
       "      <td>10783.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1</td>\n",
       "      <td>35964.70</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106.4</td>\n",
       "      <td>21.1285</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.015300e+00</td>\n",
       "      <td>0.3840</td>\n",
       "      <td>693.6</td>\n",
       "      <td>17.2752</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4259.5</td>\n",
       "      <td>1143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.2786</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-184601.81</td>\n",
       "      <td>126.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>28.5</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1706.6</td>\n",
       "      <td>1706.600</td>\n",
       "      <td>1723.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>198.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>652.7</td>\n",
       "      <td>1723.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>198.300</td>\n",
       "      <td>1</td>\n",
       "      <td>77.9</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>35.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>652.7</td>\n",
       "      <td>195.3</td>\n",
       "      <td>904.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>181.7</td>\n",
       "      <td>607.0</td>\n",
       "      <td>340318.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>457.4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>294.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>15708.18</td>\n",
       "      <td>1</td>\n",
       "      <td>624.2</td>\n",
       "      <td>0.1598</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.107100e+00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.131400e+00</td>\n",
       "      <td>1.706600e+09</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.038000e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>170000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>48.3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1556.6</td>\n",
       "      <td>27.23</td>\n",
       "      <td>10783.2</td>\n",
       "      <td>56.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>198.3</td>\n",
       "      <td>652.7</td>\n",
       "      <td>10783.20</td>\n",
       "      <td>198.300</td>\n",
       "      <td>250.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38836.0</td>\n",
       "      <td>227833.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123539.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>777.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>170000.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10831.5</td>\n",
       "      <td>10783.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1508.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1556.6</td>\n",
       "      <td>10783.20</td>\n",
       "      <td>652.7</td>\n",
       "      <td>652.7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5277923.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5243</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26824 rows × 554 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   num_1   num_2  num_3   num_4   num_5   num_6  num_7  \\\n",
       "0               0  1377.3  3712.9    NaN  1303.0  2409.0  1281.7   43.0   \n",
       "1               1    20.0    13.4    NaN   565.0     NaN   357.2   19.0   \n",
       "2               4   150.6     1.8    NaN  2294.0    15.0   107.6   76.0   \n",
       "3               5    11.4   178.0    NaN  1236.0     NaN   350.6   41.0   \n",
       "4               6  5372.0  3386.0    NaN  1340.0     NaN    14.0   45.0   \n",
       "...           ...     ...     ...    ...     ...     ...     ...    ...   \n",
       "26819       61104     NaN     NaN    NaN   108.0     NaN    85.6    4.0   \n",
       "26820       61105     4.2    97.2    NaN  2149.0   130.5   409.3   72.0   \n",
       "26821       61108  4368.0  5004.0    NaN  2215.0   478.0  3176.0   74.0   \n",
       "26822       61110     NaN     NaN    NaN   135.0     NaN    25.7    4.0   \n",
       "26823       61111    28.5    26.3    NaN    44.0   166.7  1706.6    1.0   \n",
       "\n",
       "       cat_1  num_8  num_9  num_10  num_11  num_12  num_13   num_14  num_15  \\\n",
       "0          1  195.0    NaN    27.4     NaN   417.1  2210.0  12970.8     0.0   \n",
       "1          1  170.0    NaN     NaN     NaN     NaN    68.0    123.6     2.0   \n",
       "2          1  196.0    2.3     5.3     NaN    14.5    33.0     71.6     1.0   \n",
       "3          1  117.0    5.3    25.2     NaN     NaN    39.0     57.6     2.0   \n",
       "4          1    9.0  108.0   716.0     NaN     NaN   401.0   2447.0     0.0   \n",
       "...      ...    ...    ...     ...     ...     ...     ...      ...     ...   \n",
       "26819      1  123.0    1.8     NaN     NaN     NaN    13.0      NaN     2.0   \n",
       "26820      1  157.0    NaN     NaN     NaN     NaN    72.0    775.4     1.0   \n",
       "26821      1  381.0  140.0  1041.0     NaN     NaN   223.0  11131.0     1.0   \n",
       "26822      1  151.0    NaN     3.0     NaN     NaN    83.0   1823.3     1.0   \n",
       "26823      1  104.0    2.2    75.0     NaN     NaN    96.0   1049.5     2.0   \n",
       "\n",
       "       num_16  num_17  num_18   num_19  num_20  num_21  num_22  num_23  cat_2  \\\n",
       "0       343.3     0.6     NaN  11327.5     NaN     NaN     NaN     NaN      1   \n",
       "1         NaN     NaN     NaN    168.6     NaN     NaN     NaN     NaN      1   \n",
       "2         NaN     NaN     NaN    209.1     NaN     NaN     NaN     NaN      2   \n",
       "3         NaN     1.3     NaN     56.9     NaN     NaN     NaN     NaN      1   \n",
       "4      1282.0    87.0     NaN  60925.0     NaN     NaN     NaN     NaN      1   \n",
       "...       ...     ...     ...      ...     ...     ...     ...     ...    ...   \n",
       "26819     NaN     NaN     NaN    181.4     NaN     NaN     NaN     NaN      1   \n",
       "26820     NaN     NaN     NaN    596.7     NaN     NaN     NaN     NaN      1   \n",
       "26821     NaN   940.0     NaN  14599.0     NaN     NaN     NaN     NaN      2   \n",
       "26822     NaN     NaN     NaN   1494.9     NaN     NaN     NaN     NaN      1   \n",
       "26823     0.1     NaN     NaN    652.7     NaN     NaN     NaN     NaN      2   \n",
       "\n",
       "       num_24  cat_3  cat_4  num_25  num_26   num_27  cat_5    num_28  \\\n",
       "0         NaN      1      1     NaN    60.0 -10446.6      1   559.600   \n",
       "1         NaN      1      1     NaN    12.0   -986.3      1    25.000   \n",
       "2         NaN      2      1    50.0    15.0   -161.3      1    99.800   \n",
       "3         NaN      1      1     NaN    41.0   -755.9      1   143.100   \n",
       "4         NaN      1      1     NaN    64.0   1335.0      1    28.000   \n",
       "...       ...    ...    ...     ...     ...      ...    ...       ...   \n",
       "26819     NaN      1      1     NaN    15.0   -186.4      2    96.425   \n",
       "26820     NaN      1      1     NaN     5.0   -340.7      1  -199.101   \n",
       "26821     NaN      1      1     NaN   112.0  23954.0      2   609.140   \n",
       "26822     NaN      2      1     NaN    16.0 -14274.2      1    16.800   \n",
       "26823     NaN      1      1     NaN    37.0  -9076.6      2  1706.600   \n",
       "\n",
       "            num_29  cat_6  cat_7  cat_8     num_30  cat_9  cat_10  num_31  \\\n",
       "0      18598389.68      1      1      1   74742.52      1       1     NaN   \n",
       "1        963054.61      1      1      1       0.00      2       1     NaN   \n",
       "2         71843.00      1      1      1       0.00      2       1     NaN   \n",
       "3        237787.00      1      1      1   12078.50      2       1     NaN   \n",
       "4       1665741.11      1      1      1  128520.23      2       1     NaN   \n",
       "...            ...    ...    ...    ...        ...    ...     ...     ...   \n",
       "26819    698800.00      1      1      1   17982.12      2       1    49.0   \n",
       "26820    224719.74      1      1      1   14212.11      2       1    91.4   \n",
       "26821   1588435.74      1      1      1   21098.17      3       1     NaN   \n",
       "26822   6831275.60      1      1      1       0.00      2       1     NaN   \n",
       "26823   4832346.92      1      1      1   19759.78      3       1     NaN   \n",
       "\n",
       "           num_32   num_33  num_34  num_35   num_36  num_37  num_38   num_39  \\\n",
       "0      7768369.20  559.600     NaN     NaN      NaN     NaN -1142.4      NaN   \n",
       "1        41999.36   25.000     NaN     NaN      NaN     NaN  -332.2      NaN   \n",
       "2       112589.52   20.200     1.0     NaN      NaN     NaN    -7.8      NaN   \n",
       "3        48983.65  143.100     NaN     NaN      NaN     NaN  -208.8      NaN   \n",
       "4       779486.22   28.000     5.0     NaN  53918.0   487.0    15.0  53807.0   \n",
       "...           ...      ...     ...     ...      ...     ...     ...      ...   \n",
       "26819    36157.12  113.715     NaN     NaN      NaN     NaN   -13.1      NaN   \n",
       "26820    24076.08  -77.539     1.0     NaN      0.5     NaN  -559.0      0.5   \n",
       "26821  1350208.71  921.690     NaN    10.0   8330.0    10.0   601.0   8187.0   \n",
       "26822  3083510.36   16.800     NaN     NaN      NaN     NaN    -8.9      NaN   \n",
       "26823   184601.81  198.300     NaN     NaN      NaN     NaN     NaN      NaN   \n",
       "\n",
       "       num_40     num_41  cat_11  cat_12  num_42  cat_13      num_43  num_44  \\\n",
       "0        31.0  498892.16       1       1     NaN       1  2446146.85     NaN   \n",
       "1        12.0   14982.75       1       1     NaN       1    77309.76     NaN   \n",
       "2        26.0   30734.10       1       1     NaN       1   174358.34     NaN   \n",
       "3        25.0   23600.00       1       1     NaN       1    68230.40     NaN   \n",
       "4         4.0  174363.15       1       1     NaN       1   461110.35     NaN   \n",
       "...       ...        ...     ...     ...     ...     ...         ...     ...   \n",
       "26819     NaN   10000.00       1       1     NaN       1    40000.00     NaN   \n",
       "26820    28.0   50903.89       1       1     NaN       1    13865.82     NaN   \n",
       "26821    27.0  479640.54       1       1     NaN       1  1431479.82     NaN   \n",
       "26822    25.0  199633.40       1       1     NaN       1  1180824.54     NaN   \n",
       "26823     1.0  144474.70       1       1     NaN       1   654455.30     NaN   \n",
       "\n",
       "            num_45  cat_14  cat_15  num_46  num_47  num_48  num_49     num_50  \\\n",
       "0      11306764.14       1       1     NaN     NaN     NaN     NaN  1371743.0   \n",
       "1        422913.14       2       2     NaN     NaN     NaN     NaN    54093.0   \n",
       "2         81476.86       3       1     NaN     NaN     NaN     NaN        0.0   \n",
       "3        119534.17       3       2     NaN     NaN     NaN     NaN     6935.0   \n",
       "4       1292247.39       4       1     NaN     NaN     NaN     NaN   120815.0   \n",
       "...            ...     ...     ...     ...     ...     ...     ...        ...   \n",
       "26819    342375.59       2       1     NaN     NaN     NaN     NaN        0.0   \n",
       "26820    150783.01       2       1     NaN     NaN     NaN     NaN    34821.0   \n",
       "26821  17137955.34       1       2     NaN     NaN     NaN     NaN   578525.0   \n",
       "26822   3650601.42       4       2     NaN     NaN     NaN     NaN   536039.0   \n",
       "26823   3559405.37       4       1     NaN     NaN     NaN     NaN        0.0   \n",
       "\n",
       "       num_51  num_52      num_53  num_54  cat_16  num_55  num_56  num_57  \\\n",
       "0         NaN    0.10 -1027763.45  2102.1       1    0.02     NaN   18.03   \n",
       "1         NaN    0.00   -14982.75     NaN       2    0.00     NaN   64.70   \n",
       "2         NaN    0.24   -19474.60    20.2       1    0.40     NaN   12.39   \n",
       "3         NaN    1.18   -21085.80     NaN       1    0.00     NaN   25.64   \n",
       "4         NaN    0.02   -95418.85     NaN       3    0.55     NaN   20.14   \n",
       "...       ...     ...         ...     ...     ...     ...     ...     ...   \n",
       "26819     NaN    0.00    -7871.78     NaN       1    0.10     NaN   82.60   \n",
       "26820     NaN    0.06        0.00   163.5       2    0.44     NaN   56.96   \n",
       "26821     NaN    0.10  -128258.56   663.0       1    0.33     NaN   21.37   \n",
       "26822     NaN    0.00  -412981.67     NaN       1    0.00     NaN    7.14   \n",
       "26823     NaN    0.00  -144474.70   144.6       1    0.14     NaN   32.29   \n",
       "\n",
       "       cat_17      num_58  cat_18  num_59    num_60  num_61       num_62  \\\n",
       "0           1   795055.02       1    0.08  500000.0     NaN  26672630.02   \n",
       "1           1        0.00       2    0.17   10000.0     NaN    974642.21   \n",
       "2           1    71127.00       1    0.02    3700.0     NaN         0.00   \n",
       "3           1      169.80       1    0.35   10000.0     NaN    277608.00   \n",
       "4           1  3701982.50       1    0.66       0.0     NaN   2192372.00   \n",
       "...       ...         ...     ...     ...       ...     ...          ...   \n",
       "26819       1   100000.00       1    0.16   10000.0     NaN    738801.00   \n",
       "26820       1   250873.13       1    0.60    1000.0     NaN    276854.94   \n",
       "26821       1  9258412.62       1    0.20   50000.0     NaN  17009553.20   \n",
       "26822       1        0.00       1    0.00       0.0     NaN   6823940.60   \n",
       "26823       1  1310898.90       2    0.20       0.0     NaN   3832423.92   \n",
       "\n",
       "       num_63   num_64  num_65  cat_19  num_66  num_67   num_68   num_69  \\\n",
       "0         NaN  458.400     NaN       1     NaN     0.0   7933.3   7933.3   \n",
       "1         NaN   25.000     NaN       2     NaN     0.0    168.6    168.6   \n",
       "2         NaN   16.600     NaN       3     NaN     0.0    140.8    140.8   \n",
       "3         NaN  117.400     NaN       4     NaN     0.0    -40.7    -40.7   \n",
       "4         7.0   24.000     7.0       5     NaN     1.0  59541.0  59541.0   \n",
       "...       ...      ...     ...     ...     ...     ...      ...      ...   \n",
       "26819     NaN   93.233     NaN       2     NaN     1.0    170.1    170.1   \n",
       "26820     NaN  -77.539     NaN       2     NaN     0.0    543.8    543.8   \n",
       "26821   693.0  755.440   458.0      89     NaN     0.0   8528.0   8528.0   \n",
       "26822     NaN   13.800     NaN       2     NaN     0.0     23.6     23.6   \n",
       "26823     NaN  162.600     NaN       2     NaN     0.0    457.4    457.4   \n",
       "\n",
       "       num_70   num_71  cat_20  num_72  num_73  num_74  num_75  num_76  \\\n",
       "0      6675.8   9506.6       1     0.0     NaN    22.8  1307.0     NaN   \n",
       "1       148.6    148.6       1     0.4     NaN     NaN   569.0     NaN   \n",
       "2        48.9     58.5       1     0.0     NaN     3.8  2298.0   -79.6   \n",
       "3        22.3     45.5       1     3.3     NaN    38.5  1240.0     NaN   \n",
       "4      1073.0   1473.0       1     0.0     NaN   189.0  1344.0     NaN   \n",
       "...       ...      ...     ...     ...     ...     ...     ...     ...   \n",
       "26819     0.1    181.4       1     0.1     NaN     NaN   112.0   -36.0   \n",
       "26820   489.2    571.2       1   205.2     NaN     5.2  2153.0     NaN   \n",
       "26821  6172.0  10227.0       1  3949.0     NaN  1180.0  2219.0     NaN   \n",
       "26822  1494.9   1494.9       1     0.0     NaN    30.2   139.0     NaN   \n",
       "26823   348.5    624.2       1   181.7     NaN     2.5    48.0     NaN   \n",
       "\n",
       "       cat_21  num_77  num_78  cat_22  num_79  cat_23  cat_24   num_80  \\\n",
       "0           1    17.0     NaN       1  -99.80       1       1  11728.9   \n",
       "1           2    23.0     NaN       1 -100.00       1       1   1343.5   \n",
       "2           1    23.0     NaN       1 -100.38       1       1    268.9   \n",
       "3           1    16.0     NaN       1 -112.12       1       2   1107.8   \n",
       "4           3    39.0     5.0       1 -100.19       1       2      NaN   \n",
       "...       ...     ...     ...     ...     ...     ...     ...      ...   \n",
       "26819       1    23.0     NaN       1 -100.00       1       1    321.0   \n",
       "26820       2    28.0     1.0       1 -151.52       1       1    841.4   \n",
       "26821       8    34.0     NaN       1 -127.57       1       2      NaN   \n",
       "26822       1    12.0     NaN       1 -106.56       1       4  14299.9   \n",
       "26823       1    28.0     NaN       1 -100.00       1       1  10831.5   \n",
       "\n",
       "        num_81  cat_25  num_82  cat_26  cat_27  cat_28   num_83  num_84  \\\n",
       "0      11589.0       1     NaN       1       1       1   7933.3   195.0   \n",
       "1       1318.5       1     NaN       1       2       1    168.6   170.0   \n",
       "2        248.7       1     NaN       1       2       1    140.8   196.0   \n",
       "3        964.7       1     NaN       1       2       1    -40.7   117.0   \n",
       "4          NaN       1     NaN       1       3       1  59541.0     9.0   \n",
       "...        ...     ...     ...     ...     ...     ...      ...     ...   \n",
       "26819    235.5       1     NaN       1       5       2    170.1   123.0   \n",
       "26820    899.7       1     NaN       1       2       2    543.8   157.0   \n",
       "26821      NaN       1     2.0       1       1       2   8528.0   381.0   \n",
       "26822  14283.1       1     NaN       1       2       2     23.6   151.0   \n",
       "26823  10633.2       1     NaN       1       5       2    457.4   104.0   \n",
       "\n",
       "       num_85  cat_29  num_86  num_87     num_88  num_89  num_90  cat_30  \\\n",
       "0       -25.3       1     NaN  3394.2       0.00     NaN     NaN       1   \n",
       "1         NaN       1     NaN     0.0   39927.89     NaN     NaN       1   \n",
       "2        -3.6       1     NaN    68.3   28656.88     NaN     NaN       1   \n",
       "3       -25.7       1     NaN    97.6    4773.97     NaN     NaN       1   \n",
       "4        -1.0       1     NaN  1384.0       0.00     NaN     NaN       1   \n",
       "...       ...     ...     ...     ...        ...     ...     ...     ...   \n",
       "26819   -15.4       1     NaN    11.3   63477.91     NaN     NaN       1   \n",
       "26820     NaN       1     NaN    52.9  101791.00     NaN     NaN       1   \n",
       "26821  -125.0       1     NaN  6071.0  106477.00     NaN     NaN       1   \n",
       "26822    -3.0       1     NaN  1471.3   15206.83     NaN     NaN       1   \n",
       "26823   -35.7       1     NaN   195.3  162861.18     NaN     NaN       1   \n",
       "\n",
       "       num_91  cat_31  cat_32    num_92  num_93  num_94  num_95  num_96  \\\n",
       "0         NaN       1       1  46913.20     NaN  2898.9     0.0     NaN   \n",
       "1         NaN       1       1   1343.50     NaN   168.4     0.0     NaN   \n",
       "2         NaN       1       1    268.90     NaN    19.2     0.0     NaN   \n",
       "3         NaN       1       1   1106.50     NaN   -54.5     0.0     NaN   \n",
       "4         NaN       1       1   5396.00     NaN   202.0     0.0     NaN   \n",
       "...       ...     ...     ...       ...     ...     ...     ...     ...   \n",
       "26819     NaN       1       1    361.76     NaN    70.1     1.0     NaN   \n",
       "26820     NaN       1       1    997.50     NaN   543.3     3.0     NaN   \n",
       "26821     NaN       1       1  36082.90     NaN     NaN     0.0    39.0   \n",
       "26822     NaN       1       1  14299.90     NaN    23.5     0.0     NaN   \n",
       "26823     NaN       1       1  10783.20     NaN   257.4     0.0     NaN   \n",
       "\n",
       "       num_97  num_98  num_99  num_100  num_101  cat_33  num_102  num_103  \\\n",
       "0         5.0     0.0     NaN      NaN      NaN       1  50000.0      NaN   \n",
       "1        12.0     NaN     NaN      NaN      NaN       1      0.0      NaN   \n",
       "2        11.0    12.0     NaN      NaN      NaN       1   3700.0      NaN   \n",
       "3        15.0    15.0     NaN      NaN      NaN       1  23600.0      NaN   \n",
       "4        11.0   104.0     NaN      5.0      NaN       1   1000.0      NaN   \n",
       "...       ...     ...     ...      ...      ...     ...      ...      ...   \n",
       "26819    22.0     NaN     NaN      NaN      NaN       1      0.0      NaN   \n",
       "26820    84.0     NaN     NaN      1.0      NaN       1   1000.0      NaN   \n",
       "26821     0.0     0.0     NaN      NaN      NaN       1  50000.0      NaN   \n",
       "26822     8.0     NaN     NaN      NaN      NaN       1      0.0      NaN   \n",
       "26823    15.0     NaN     NaN      NaN      NaN       1      0.0      NaN   \n",
       "\n",
       "       num_104    num_105  num_106      num_107  num_108  num_109  num_110  \\\n",
       "0         33.3 -440123.53      NaN  28333733.29      NaN      NaN   3303.2   \n",
       "1          NaN   -2636.77      NaN    921779.91      NaN      NaN      0.0   \n",
       "2          6.4  -13634.77      NaN    176490.80      NaN      NaN     68.3   \n",
       "3         18.4   -7901.81      NaN    307216.11      NaN      NaN     97.6   \n",
       "4        277.0  -11802.15      NaN   7671415.89      NaN      NaN   1384.0   \n",
       "...        ...        ...      ...          ...      ...      ...      ...   \n",
       "26819      9.5    -281.13      NaN    975317.87      NaN      NaN     11.3   \n",
       "26820      9.5    -292.40      NaN    572621.73      NaN      NaN     52.9   \n",
       "26821    535.0  -19617.18      NaN  28391614.65      NaN      NaN   6071.0   \n",
       "26822     32.5 -178269.65      NaN   7441945.93      NaN      NaN   1471.3   \n",
       "26823      8.1  -55529.92      NaN   8664194.25      NaN      NaN    195.3   \n",
       "\n",
       "       num_111  num_112  cat_34  num_113  num_114  num_115  cat_35  num_116  \\\n",
       "0       3303.2    28.68       1  11327.5   1820.9      0.0       1      NaN   \n",
       "1          0.0    50.33       1    168.6     20.0      1.0       1      NaN   \n",
       "2         68.3    40.56       1    209.1    150.6      0.0       1      NaN   \n",
       "3         97.6    57.45       1     56.9     11.4      1.0       1      NaN   \n",
       "4       1384.0    49.64       1  60925.0  59452.0      0.0       1   1349.0   \n",
       "...        ...      ...     ...      ...      ...      ...     ...      ...   \n",
       "26819     11.3    99.39       2    181.4      0.0      0.0       1      NaN   \n",
       "26820     52.9    57.85       2    596.7     25.5      1.0       1      NaN   \n",
       "26821   6071.0    27.96       2  14599.0   4372.0      0.0       1  27130.0   \n",
       "26822   1471.3     6.99       2   1494.9      0.0      0.0       1      NaN   \n",
       "26823    195.3    56.06       2    652.7     28.5      0.0       1      NaN   \n",
       "\n",
       "       num_117  num_118     num_119  cat_36    num_120  cat_37  num_121  \\\n",
       "0       7933.3      NaN  2544058.39       1  500000.00       1      NaN   \n",
       "1        168.6      NaN   158062.81       1   35000.00       1      NaN   \n",
       "2        140.8      NaN     5183.07       1   31000.00       1      NaN   \n",
       "3        -40.7      NaN   109335.32       1   23600.00       1      NaN   \n",
       "4      59541.0     79.0  5064200.58       1  190000.00       1      NaN   \n",
       "...        ...      ...         ...     ...        ...     ...      ...   \n",
       "26819    170.1      NaN   159050.04       3   10000.00       1      NaN   \n",
       "26820    543.8      NaN   347079.90       2   62500.00       1      NaN   \n",
       "26821   8528.0   2642.0  5869403.31       2  489783.82       1      NaN   \n",
       "26822     23.6      NaN    13105.43       2  200000.00       1      NaN   \n",
       "26823    457.4      NaN  1742451.40       2  170000.00       1      NaN   \n",
       "\n",
       "       num_122  cat_38  cat_39  num_123  num_124  num_125  num_126  num_127  \\\n",
       "0          0.0       1       1      1.0  11728.3      NaN      NaN      NaN   \n",
       "1          0.0       2       2      2.0   1343.5      NaN      NaN      NaN   \n",
       "2          0.0       3       2      1.0    268.9      NaN      NaN      NaN   \n",
       "3          0.0       4       3      2.0   1106.5      NaN      NaN      NaN   \n",
       "4          0.0       5       3      3.0   1349.0      NaN      NaN    435.0   \n",
       "...        ...     ...     ...      ...      ...      ...      ...      ...   \n",
       "26819      0.0      77       3      1.0    272.0      NaN      NaN      NaN   \n",
       "26820      0.0     172       2      2.0    750.0      NaN      NaN      NaN   \n",
       "26821      0.0      45       1      1.0  27130.0      NaN      NaN      NaN   \n",
       "26822      0.0      35       2      1.0  14299.9      NaN      NaN      NaN   \n",
       "26823      0.0      89       1      1.0  10783.2      NaN      NaN      NaN   \n",
       "\n",
       "            num_128  num_129   num_130  num_131  num_132  num_133  num_134  \\\n",
       "0      5.517000e-01   2270.0  39184.65      NaN    224.0      0.0   6728.0   \n",
       "1      2.500000e+07    967.0      0.00      NaN     32.0      0.0    967.0   \n",
       "2      1.496200e+00   2297.0      0.00      NaN    106.0      0.0   3167.0   \n",
       "3      1.431000e+08   1244.0      0.00      NaN     42.0      0.0   1247.0   \n",
       "4      9.032000e-01   2203.0      0.00      NaN    147.0      0.0   4408.0   \n",
       "...             ...      ...       ...      ...      ...      ...      ...   \n",
       "26819  1.137150e+08    268.0      0.00      NaN      9.0      0.0    268.0   \n",
       "26820 -7.753900e+07   2514.0      0.00      NaN    130.0      0.0   3886.0   \n",
       "26821  9.216900e+08   2515.0      0.00    235.0    189.0      0.0   5663.0   \n",
       "26822  8.560000e-02    740.0      0.00      NaN     25.0      0.0    741.0   \n",
       "26823  1.983000e+08    603.0      0.00      NaN     20.0      0.0    603.0   \n",
       "\n",
       "       cat_40  cat_41  cat_42  num_135  cat_43  num_136  num_137  num_138  \\\n",
       "0           1       1       1      NaN       1   1524.3   3565.9      NaN   \n",
       "1           2       1       1      NaN       2     20.0     13.4      NaN   \n",
       "2           2       1       1      NaN       1    150.6      1.8      NaN   \n",
       "3           2       1       1      NaN       1     12.8    176.6      NaN   \n",
       "4           2       1       1      NaN       1   5111.0   3377.0      NaN   \n",
       "...       ...     ...     ...      ...     ...      ...      ...      ...   \n",
       "26819       2       1       1      NaN       2      NaN      NaN      NaN   \n",
       "26820       2       1       1      NaN       1      4.5     96.9      NaN   \n",
       "26821       1       1       1      NaN       2   4554.0   4438.0      NaN   \n",
       "26822       2       1       1      NaN       2      NaN      NaN      NaN   \n",
       "26823       1       1       1      NaN       2     22.9      9.5      NaN   \n",
       "\n",
       "       num_139  cat_44  cat_45  cat_46  num_140  num_141  cat_47  num_142  \\\n",
       "0          0.0       1       1       1      NaN   5034.4       1      NaN   \n",
       "1          0.0       1       2       1      NaN      0.2       1      NaN   \n",
       "2          0.0       1       1       1      NaN    121.6       1      NaN   \n",
       "3          0.0       1       2       1      NaN     13.8       1      NaN   \n",
       "4          0.0       1       1       1      NaN   4661.0       1      NaN   \n",
       "...        ...     ...     ...     ...      ...      ...     ...      ...   \n",
       "26819      0.0       1       2       1      NaN      NaN       1      NaN   \n",
       "26820      0.0       1       2       1      NaN      NaN       1      NaN   \n",
       "26821      0.0       1       1       1      NaN    120.0       1      NaN   \n",
       "26822      0.0       1       3       1      NaN      0.1       1      NaN   \n",
       "26823      0.0       1       3       1      NaN    200.0       1      NaN   \n",
       "\n",
       "       num_143  num_144  cat_48  cat_49  cat_50  num_145  num_146  num_147  \\\n",
       "0          NaN      3.0       1       1       1     0.03      NaN   2830.8   \n",
       "1          NaN      2.0       1       1       1     0.00      NaN      NaN   \n",
       "2          NaN      2.0       1       1       1     0.98      NaN      9.6   \n",
       "3          NaN      3.0       1       1       1     0.00      NaN     23.2   \n",
       "4          NaN      3.0       1       1       2     0.00   0.8849    400.0   \n",
       "...        ...      ...     ...     ...     ...      ...      ...      ...   \n",
       "26819      NaN      2.0       1       1       1     0.00      NaN    181.3   \n",
       "26820      NaN      2.0       1       1       1     0.15      NaN     82.0   \n",
       "26821      NaN      5.0       1       1       1     0.21   0.0327   4012.0   \n",
       "26822      NaN      1.0       1       1       1     0.00      NaN      NaN   \n",
       "26823      NaN      2.0       1       1       2     0.00      NaN    275.7   \n",
       "\n",
       "       num_148  num_149  num_150  cat_51  num_151  cat_52  num_152  cat_53  \\\n",
       "0      -1112.0      NaN      NaN       1     0.93       1      NaN       1   \n",
       "1       -399.0      NaN      NaN       1      NaN       1      NaN       1   \n",
       "2      -2102.0      NaN      NaN       1     1.24       1      NaN       1   \n",
       "3      -1123.0      NaN      NaN       1     1.02       1      NaN       1   \n",
       "4      -1335.0    500.0      NaN       1     1.34       1      NaN       1   \n",
       "...        ...      ...      ...     ...      ...     ...      ...     ...   \n",
       "26819     11.0      NaN      NaN       1      NaN       1      NaN       1   \n",
       "26820  -1996.0     50.0      NaN       1     0.73       1      NaN       1   \n",
       "26821  -1838.0      NaN      NaN       1     1.31       1      NaN       1   \n",
       "26822     12.0      NaN      NaN       1      NaN       1      NaN       1   \n",
       "26823     56.0      NaN      NaN       1      NaN       1      NaN       1   \n",
       "\n",
       "       num_153    num_154  num_155  num_156  num_157  cat_54      num_158  \\\n",
       "0          NaN  3836906.0   6774.4      NaN      NaN       1  27276369.89   \n",
       "1          NaN   142472.0    121.7      NaN      NaN       1    279426.28   \n",
       "2         50.0    18440.0      NaN      NaN      NaN       1    150412.34   \n",
       "3          NaN    42092.0      1.2      NaN      NaN       1    165541.67   \n",
       "4        500.0   114630.0    254.0      NaN      NaN       1   2469064.87   \n",
       "...        ...        ...      ...      ...      ...     ...          ...   \n",
       "26819      NaN    23250.0      NaN      NaN      NaN       1    747785.45   \n",
       "26820     50.0    36676.0    258.2      NaN      NaN       1    144860.97   \n",
       "26821      NaN  3507947.0   1922.0      NaN      NaN       1  15455195.30   \n",
       "26822      NaN   225754.0   1504.3      NaN      NaN       1   7340792.24   \n",
       "26823      NaN   278838.0      NaN      NaN      NaN       1   3188437.05   \n",
       "\n",
       "       num_159  num_160    num_161  num_162  num_163  num_164  num_165  \\\n",
       "0         0.65     50.0  500000.00     0.00      NaN      0.0   0.2414   \n",
       "1         0.97      NaN   35000.00    10.78      NaN      0.0   0.1254   \n",
       "2         0.40     46.0   15000.00  3493.65      NaN      0.0   0.7776   \n",
       "3         0.79     49.0   20000.00     0.00      NaN      0.0   0.0514   \n",
       "4         0.24     54.0   35000.00     0.00      NaN      0.0  11.2907   \n",
       "...        ...      ...        ...      ...      ...      ...      ...   \n",
       "26819     0.72      NaN   10000.00     0.00      NaN      1.0   0.5014   \n",
       "26820     0.40     55.0   15000.00     0.00      NaN      0.0   0.5981   \n",
       "26821     0.05      NaN  489783.82    27.26      NaN      0.0   0.4045   \n",
       "26822     0.94      NaN  200000.00     0.00      NaN     -0.0   0.1045   \n",
       "26823     0.55      NaN  170000.00     0.00      NaN      1.0   0.0605   \n",
       "\n",
       "       num_166  num_167  num_168  cat_55    num_169  num_170  num_171  \\\n",
       "0          NaN      2.0      0.0       1  377973.06    923.2      NaN   \n",
       "1          NaN      0.0     15.0       1    1410.33      NaN      NaN   \n",
       "2          NaN      0.0      9.0       1   12156.37     13.5      NaN   \n",
       "3          NaN      0.0      0.0       1    2224.14      NaN      NaN   \n",
       "4          NaN      1.0      0.0       1   27896.32     31.0      NaN   \n",
       "...        ...      ...      ...     ...        ...      ...      ...   \n",
       "26819      NaN      0.0     23.0       1     710.00      NaN      NaN   \n",
       "26820      NaN      7.0      0.0       1     350.52      NaN      NaN   \n",
       "26821      NaN      1.0      1.0       1   37228.78      NaN      NaN   \n",
       "26822      NaN      0.0     -1.0       1  118279.54    196.2      NaN   \n",
       "26823      NaN      1.0     16.0       1   21710.18      NaN      NaN   \n",
       "\n",
       "       num_172  num_173  cat_56       num_174       num_175  num_176  num_177  \\\n",
       "0       2270.0      9.0       1  4.356800e+00  7.822200e+00   0.7003      4.5   \n",
       "1        967.0      8.0       1  8.430000e+00  1.686000e+08   1.0000      NaN   \n",
       "2       2297.0     15.0       1  9.349000e-01  1.042960e+01   0.6733      NaN   \n",
       "3       1244.0     11.0       1 -3.570100e+00 -4.070000e+07  -0.7152      6.1   \n",
       "4       2203.0     27.0       1  1.001400e+00  1.920677e+03   0.9772    102.0   \n",
       "...        ...      ...     ...           ...           ...      ...      ...   \n",
       "26819    268.0     20.0       1  1.701000e+08  1.701000e+08   0.9377      NaN   \n",
       "26820   2514.0     25.0       1  2.132540e+01  5.438000e+08   0.9113      0.6   \n",
       "26821   2515.0     22.0       1  1.950500e+00  8.528000e+09   0.5841      NaN   \n",
       "26822    740.0      7.0       1  2.360000e+07  1.202000e-01   0.0157      NaN   \n",
       "26823    603.0     22.0       1  1.604910e+01  4.574000e+08   0.7007      0.7   \n",
       "\n",
       "       num_178      num_179       num_180       num_181  num_182  num_183  \\\n",
       "0       0.0494       0.0000  2.021000e+00  2.877900e+00      NaN   0.0577   \n",
       "1       0.1482  400000.0000  1.486000e+08  1.486000e+08      NaN   0.1482   \n",
       "2       0.4772       0.0000  7.159000e-01  8.565000e-01      NaN   0.1178   \n",
       "3       2.5149       0.0338  2.284000e-01  4.661000e-01      NaN  -2.8845   \n",
       "4       0.0004       0.0000  7.752000e-01  1.064300e+00   0.0051   0.0004   \n",
       "...        ...          ...           ...           ...      ...      ...   \n",
       "26819   0.5315       0.0088  8.800000e-03  1.605300e+01      NaN   0.5481   \n",
       "26820  -0.3336       3.8790  9.247600e+00  1.079770e+01      NaN  -0.1425   \n",
       "26821   0.0417       0.6504  1.016600e+00  1.684500e+00   0.0255   0.0885   \n",
       "26822   0.0112       0.0000  1.016000e+00  1.016000e+00      NaN   0.5847   \n",
       "26823   2.6146       0.9303  1.784400e+00  3.196100e+00      NaN   0.3554   \n",
       "\n",
       "       num_184  cat_57  num_185  cat_58  num_186  num_187  num_188  cat_59  \\\n",
       "0          0.0       1      1.0       1  14938.7     25.0     0.00       1   \n",
       "1          1.0       2      1.0       1    143.6     22.0  7405.74       1   \n",
       "2          0.0       1      1.0       1    222.2     21.0    26.84       2   \n",
       "3          1.0       3      0.0       1     70.4      5.0     0.00       1   \n",
       "4          0.0       4      0.0       1  61800.0      8.0     0.00       2   \n",
       "...        ...     ...      ...     ...      ...      ...      ...     ...   \n",
       "26819      1.0       2      4.0       1      NaN    106.0     0.00       2   \n",
       "26820      1.0       7      2.0       1    801.2     63.0     0.00       2   \n",
       "26821      1.0       2      4.0       1  15793.0    107.0  1959.67       2   \n",
       "26822      1.0       2      2.0       1   1823.3     46.0     0.00       1   \n",
       "26823      1.0       2      0.0       1   1072.4      1.0     0.00       2   \n",
       "\n",
       "       cat_60  num_189  cat_61  num_190  num_191  num_192  num_193  \\\n",
       "0           1      NaN       1      NaN      2.0     0.93     59.0   \n",
       "1           2      NaN       1      NaN      3.0      NaN      NaN   \n",
       "2           3      NaN       1      NaN      2.0     0.98     32.5   \n",
       "3           4      NaN       1      NaN      1.0      NaN      6.5   \n",
       "4           3      NaN       1      NaN      4.0     0.11      NaN   \n",
       "...       ...      ...     ...      ...      ...      ...      ...   \n",
       "26819    1313      NaN       1      NaN      1.0      NaN      NaN   \n",
       "26820     915      NaN       1      NaN      4.0      NaN     19.4   \n",
       "26821     914      NaN       1      NaN      4.0      NaN    648.0   \n",
       "26822     423      NaN       1      NaN      1.0      NaN      NaN   \n",
       "26823     252      NaN       1      NaN      4.0      NaN      3.6   \n",
       "\n",
       "          num_194  num_195  num_196  num_197  num_198  cat_62  num_199  \\\n",
       "0      1438784.73      NaN   3303.2      9.0      NaN       1      NaN   \n",
       "1        14982.75      NaN      NaN      8.0      NaN       1      NaN   \n",
       "2        18260.27      NaN     68.3     16.0      NaN       1     50.0   \n",
       "3        18095.07      NaN     97.6      8.0      NaN       1      NaN   \n",
       "4        73359.78      NaN   1384.0     20.0      NaN       1      NaN   \n",
       "...           ...      ...      ...      ...      ...     ...      ...   \n",
       "26819     7871.78      NaN     11.3      1.0      NaN       3      NaN   \n",
       "26820     8187.35      NaN     52.9     10.0      NaN       2      NaN   \n",
       "26821   420096.36      NaN   6071.0     10.0      NaN       2      NaN   \n",
       "26822   602862.34      NaN   1471.3      1.0      NaN       2      NaN   \n",
       "26823    16808.64      NaN    195.3      1.0      NaN       2      NaN   \n",
       "\n",
       "        num_200  num_201  cat_63  num_202  cat_64  cat_65  cat_66  num_203  \\\n",
       "0      616000.0      1.0       1     0.96       1       1       1      4.0   \n",
       "1           0.0      1.0       1     0.30       1       2       1      3.0   \n",
       "2       71000.0      0.0       1     0.85       1       2       1      4.0   \n",
       "3           0.0      1.0       1     0.53       1       1       1      4.0   \n",
       "4       13000.0      1.0       1     0.32       1       2       1      7.0   \n",
       "...         ...      ...     ...      ...     ...     ...     ...      ...   \n",
       "26819       0.0      1.0       1     0.76       1       2       1      6.0   \n",
       "26820   35500.0      1.0       1     0.25       1       2       1      3.0   \n",
       "26821  344500.0      1.0       1     0.54       1       4       1      6.0   \n",
       "26822       0.0      0.0       1     0.98       1       2       1      2.0   \n",
       "26823       0.0      1.0       1     0.36       1       3       1      5.0   \n",
       "\n",
       "       num_204  num_205  num_206  num_207  num_208  cat_67  cat_68  num_209  \\\n",
       "0          NaN      1.0      9.0      NaN      NaN       1       1   0.1092   \n",
       "1          NaN      2.0     16.0      NaN      NaN       2       2   0.2658   \n",
       "2          NaN      1.0     15.0      1.0      NaN       2       2   0.4001   \n",
       "3          NaN      1.0     19.0      NaN      NaN       1       2   0.3168   \n",
       "4      23500.0      1.0     15.0      NaN      NaN       1       2   0.0103   \n",
       "...        ...      ...      ...      ...      ...     ...     ...      ...   \n",
       "26819      NaN      1.0     26.0      NaN      NaN       1       2   0.3147   \n",
       "26820      NaN      1.0     88.0      NaN      NaN       2       2   0.5457   \n",
       "26821      NaN      1.0      4.0      NaN      NaN       1       2   0.1170   \n",
       "26822      NaN      1.0     12.0      NaN      NaN       2       2   0.0017   \n",
       "26823      0.0      1.0     19.0      NaN      NaN       1       2   0.1582   \n",
       "\n",
       "       num_210  cat_69  cat_70     num_211  cat_71  num_212  num_213  num_214  \\\n",
       "0          NaN       1       1  4694287.90       1      0.0      1.0     76.0   \n",
       "1          NaN       2       1   163857.03       1      0.0      1.0     32.0   \n",
       "2          NaN       3       1    29390.00       1      1.0      0.0     77.0   \n",
       "3          NaN       4       1    49735.46       1      0.0      3.0     41.0   \n",
       "4          NaN       1       1   890391.86       1      0.0      5.0     73.0   \n",
       "...        ...     ...     ...         ...     ...      ...      ...      ...   \n",
       "26819      NaN       7       2   161316.83       1      0.0      4.0      9.0   \n",
       "26820      NaN       7       2    75521.34       1      0.0      2.0     84.0   \n",
       "26821      NaN       7       2  3081589.34       1      1.0      2.0     84.0   \n",
       "26822      NaN       7       2  1207212.60       1      0.0      2.0     25.0   \n",
       "26823      NaN       7       2  1019637.13       1      1.0      1.0     20.0   \n",
       "\n",
       "       num_215    num_216  num_217  num_218  num_219  num_220  num_221  \\\n",
       "0          NaN   10214.74      NaN      4.0   0.0216     23.0    297.0   \n",
       "1          NaN    7058.93      NaN     19.0   0.0000      0.0      NaN   \n",
       "2          NaN      46.98      NaN     13.0   0.0502      0.0     33.9   \n",
       "3          NaN    1310.65      NaN      4.0   0.0000      6.0      NaN   \n",
       "4          NaN    8634.85      NaN      4.0   0.0057     76.0    742.0   \n",
       "...        ...        ...      ...      ...      ...      ...      ...   \n",
       "26819      NaN   43095.40      NaN     27.0   0.0000      6.0      NaN   \n",
       "26820      NaN    4703.38      NaN      4.0   0.0000      3.0      NaN   \n",
       "26821      NaN  101808.34      NaN      5.0   0.0000      3.0      NaN   \n",
       "26822      NaN   14250.34      NaN      3.0   0.0137      0.0      NaN   \n",
       "26823      NaN   43220.74      NaN     20.0   0.0000      4.0      NaN   \n",
       "\n",
       "       num_222  num_223  num_224    num_225  cat_72  cat_73  num_226  num_227  \\\n",
       "0          NaN      NaN      NaN   31425.20       1       1    297.0      NaN   \n",
       "1          NaN      NaN      NaN    1601.78       1       2      NaN      NaN   \n",
       "2          NaN      NaN      NaN     495.39       2       3     41.2      NaN   \n",
       "3          NaN      NaN      NaN   14037.20       1       4      NaN      NaN   \n",
       "4          NaN      NaN      NaN   18251.11       1       4    303.0      NaN   \n",
       "...        ...      ...      ...        ...     ...     ...      ...      ...   \n",
       "26819      NaN      NaN      NaN    9813.49       1       4      NaN      NaN   \n",
       "26820      NaN      NaN      NaN    1983.28       1       8      NaN      NaN   \n",
       "26821      NaN      NaN      NaN    3866.24       1       6      NaN      NaN   \n",
       "26822      NaN      NaN      NaN  177213.76       1       3      NaN      NaN   \n",
       "26823      NaN      NaN      NaN  141797.66       1       4      NaN      NaN   \n",
       "\n",
       "           num_228  cat_74  num_229  num_230  num_231  num_232  cat_75  \\\n",
       "0      28344782.47       1     14.0      1.0    90.65   2274.0       1   \n",
       "1        988142.21       1      NaN      1.0    34.72    971.0       1   \n",
       "2        176340.03       1     18.0      0.0    71.82   2301.0       1   \n",
       "3        298412.80       1     57.0      0.0    36.05   1248.0       2   \n",
       "4       6733855.70       1    195.0      2.0    53.36   2207.0       1   \n",
       "...            ...     ...      ...      ...      ...      ...     ...   \n",
       "26819    967901.00       1      NaN      1.0    82.78    272.0       2   \n",
       "26820    557728.07       1      5.0      0.0    74.36   2518.0       1   \n",
       "26821  27824618.73       1      NaN      1.0    43.98   2519.0       1   \n",
       "26822   7243275.60       1      NaN      0.0    94.61    744.0       1   \n",
       "26823   8742870.92       1      NaN      0.0    26.04    607.0       1   \n",
       "\n",
       "       num_233  num_234  cat_76  num_235  cat_77  num_236  num_237    num_238  \\\n",
       "0         0.00   2270.0       1     76.0       1      NaN   1967.9  1033242.0   \n",
       "1         0.00    955.0       1     32.0       2      NaN     20.0        0.0   \n",
       "2         0.00   2297.0       1     77.0       3      NaN    150.6     3504.0   \n",
       "3         0.00   1244.0       1     41.0       4      NaN     12.8     8468.0   \n",
       "4        52.13   2203.0       1     73.0       5  53807.0  59353.0   146000.0   \n",
       "...        ...      ...     ...      ...     ...      ...      ...        ...   \n",
       "26819     0.00    119.0       1      4.0       2      NaN      NaN    66174.5   \n",
       "26820     0.00   2514.0       1     84.0       2     21.3     25.8    29930.0   \n",
       "26821     0.00   2515.0       1     84.0       7      NaN   4662.0  1480075.0   \n",
       "26822     0.00    147.0       1      5.0       2      NaN      NaN        0.0   \n",
       "26823    10.70    100.0       1      3.0       2      NaN     22.9   100630.5   \n",
       "\n",
       "       cat_78  num_239  cat_79    num_240  num_241  num_242  cat_80  num_243  \\\n",
       "0           1  14938.7       1   22123.83      NaN      NaN       1      NaN   \n",
       "1           1    143.6       1   39927.89      NaN      NaN       1      NaN   \n",
       "2           1    222.2       1   43897.88      NaN      NaN       1      NaN   \n",
       "3           1     70.4       1    4773.97      NaN      NaN       1      NaN   \n",
       "4           1  61800.0       1    4861.79      NaN      NaN       1      NaN   \n",
       "...       ...      ...     ...        ...      ...      ...     ...      ...   \n",
       "26819       1      NaN       1   63477.91      NaN      NaN       1      NaN   \n",
       "26820       1    801.2       1  121256.29      NaN      NaN       1      NaN   \n",
       "26821       1  15793.0       1  144088.47      NaN      NaN       1      NaN   \n",
       "26822       1   1823.3       1   15206.83      NaN      NaN       1      NaN   \n",
       "26823       1   1072.4       1  162861.18      NaN      NaN       1      NaN   \n",
       "\n",
       "       num_244  cat_81  cat_82  num_245  cat_83  num_246  num_247  num_248  \\\n",
       "0        114.6       1       1     91.0       1   1014.2      NaN      NaN   \n",
       "1         25.0       2       1      NaN       2      0.0      NaN      NaN   \n",
       "2         16.6       2       1      NaN       1     13.5      NaN      NaN   \n",
       "3        117.4       2       1      NaN       2      0.0      NaN      NaN   \n",
       "4          6.0       1       2      NaN       2     31.0     10.0      NaN   \n",
       "...        ...     ...     ...      ...     ...      ...      ...      ...   \n",
       "26819     70.1       2       1      NaN       1      0.0      NaN      NaN   \n",
       "26820    -58.3       2       1      NaN       2      0.0      NaN      NaN   \n",
       "26821    568.0       1       2      NaN       1      0.0    739.0      NaN   \n",
       "26822     13.8       2       1      NaN       1    196.2      NaN      NaN   \n",
       "26823    162.6       2       1      NaN       1      0.0      NaN      NaN   \n",
       "\n",
       "       num_249  num_250  num_251  num_252  num_253  num_254  num_255  num_256  \\\n",
       "0          NaN      NaN    443.6      2.0      NaN      NaN  -139.99      NaN   \n",
       "1          NaN      NaN      NaN      2.0      NaN      NaN  -100.00      NaN   \n",
       "2          NaN      NaN      NaN      2.0      NaN      NaN   -93.76      NaN   \n",
       "3          NaN      NaN      NaN      2.0      NaN      NaN   -85.81      NaN   \n",
       "4          NaN      NaN    435.0      3.0      NaN      NaN   -76.88      NaN   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "26819      NaN      NaN      NaN      2.0      NaN      NaN  -100.00      NaN   \n",
       "26820      NaN      NaN      NaN      2.0      NaN      NaN      NaN      NaN   \n",
       "26821      NaN      NaN    106.0      2.0      NaN      NaN  -327.53      NaN   \n",
       "26822      NaN      NaN      NaN      2.0      NaN      NaN  -145.97      NaN   \n",
       "26823      NaN      NaN      NaN      1.0      NaN      NaN   -11.63      NaN   \n",
       "\n",
       "       cat_84  cat_85  num_257  cat_86  num_258  num_259  cat_87  cat_88  \\\n",
       "0           1       1      NaN       1      NaN      0.0       1       1   \n",
       "1           1       1      NaN       1      NaN      0.0       1       1   \n",
       "2           1       1      NaN       1      NaN      0.0       1       1   \n",
       "3           1       1      NaN       1      NaN      0.0       1       1   \n",
       "4           1       1      NaN       1      NaN      0.0       1       1   \n",
       "...       ...     ...      ...     ...      ...      ...     ...     ...   \n",
       "26819       1       1      NaN       1      NaN      0.0       1       1   \n",
       "26820       2       1      NaN       1      NaN      0.0       1       1   \n",
       "26821       1       1      NaN       1      NaN      0.0       2       1   \n",
       "26822       1       1      NaN       1      NaN      0.0       1       1   \n",
       "26823       1       3      NaN       1      NaN      0.0       1       1   \n",
       "\n",
       "        num_260  num_261  num_262  cat_89  num_263  cat_90  cat_91  cat_92  \\\n",
       "0        324.57      NaN      0.5       1      0.0       1       1       1   \n",
       "1          1.80      NaN      NaN       1      0.0       2       2       1   \n",
       "2       5402.10      NaN      NaN       1      0.0       2       1       1   \n",
       "3      15205.80      NaN      NaN       1      0.0       2       1       1   \n",
       "4       2052.20      NaN      NaN       1      0.0       2       1       1   \n",
       "...         ...      ...      ...     ...      ...     ...     ...     ...   \n",
       "26819    168.00      NaN      NaN       1      0.0       2       2       2   \n",
       "26820   1122.24      NaN     58.1       1      0.0       2       1       2   \n",
       "26821    173.40      NaN    160.0       2      0.0       2       1       2   \n",
       "26822    303.48      NaN     26.3       1      0.0       2       1       2   \n",
       "26823       NaN      NaN      NaN       1      0.0       2       2       2   \n",
       "\n",
       "        num_264  num_265  num_266  num_267    num_268    num_269   num_270  \\\n",
       "0      46913.20   9506.6      NaN  11327.5  41786.400  41786.400  46913.20   \n",
       "1       1343.50    148.6      NaN    168.6    986.300    986.300   1343.50   \n",
       "2        268.90     58.5      NaN    209.1    161.300    161.300    268.90   \n",
       "3       1106.50     45.5      NaN     56.9    755.900    755.900   1106.50   \n",
       "4       5396.00   1473.0      NaN  60925.0   5340.000   5340.000   5396.00   \n",
       "...         ...      ...      ...      ...        ...        ...       ...   \n",
       "26819    361.76    181.4      NaN    181.4    247.912    247.912    361.76   \n",
       "26820    997.50    571.2      NaN    596.7    453.131    453.131    997.50   \n",
       "26821  36082.90  10227.0      NaN  14599.0  31858.820  31858.820  36082.90   \n",
       "26822  14299.90   1494.9      NaN   1494.9  14274.200  14274.200  14299.90   \n",
       "26823  10783.20    624.2      NaN    652.7   9076.600   9076.600  10783.20   \n",
       "\n",
       "       num_271  cat_93  num_272  num_273  num_274  cat_94  cat_95  num_275  \\\n",
       "0          NaN       1      NaN      NaN      NaN       1       1   2163.0   \n",
       "1          NaN       1      NaN      NaN      NaN       1       1     64.0   \n",
       "2          NaN       1      NaN      NaN      NaN       1       1      0.0   \n",
       "3          NaN       1     30.8      NaN      NaN       1       1     38.0   \n",
       "4          NaN       1      NaN  60925.0      NaN       1       1    249.0   \n",
       "...        ...     ...      ...      ...      ...     ...     ...      ...   \n",
       "26819      NaN       1      NaN      NaN      NaN       1       1     10.0   \n",
       "26820      NaN       1     48.4      NaN      NaN       1       1     43.0   \n",
       "26821      NaN       1     20.0  14599.0      NaN       1       1    213.0   \n",
       "26822      NaN       1      7.7      NaN      NaN       1       1     79.0   \n",
       "26823      NaN       1      NaN      NaN      NaN       1       1     78.0   \n",
       "\n",
       "       cat_96    num_276  num_277  num_278  num_279  cat_97  num_280  num_281  \\\n",
       "0           1  413050.73      6.0    443.6      NaN       1      NaN      NaN   \n",
       "1           1    6390.78      6.0      NaN      NaN       2      NaN      NaN   \n",
       "2           1   13412.18      6.0      NaN      NaN       1      NaN      NaN   \n",
       "3           1    5589.90      5.0      NaN      NaN       1      NaN      NaN   \n",
       "4           1    2052.10     11.0    162.0      NaN       1      NaN      NaN   \n",
       "...       ...        ...      ...      ...      ...     ...      ...      ...   \n",
       "26819       1    7871.78      7.0      NaN      NaN       2      NaN      NaN   \n",
       "26820       1    3333.07      7.0      NaN      NaN       1      NaN      NaN   \n",
       "26821       1  291837.80     10.0      4.0      NaN       2      NaN      NaN   \n",
       "26822       1  195532.89      3.0      NaN      NaN       2      NaN      NaN   \n",
       "26823       1   35964.70      9.0      NaN      NaN       2      NaN      NaN   \n",
       "\n",
       "       num_282   num_283  num_284   num_285  num_286       num_287  num_288  \\\n",
       "0       2260.3   73.9644  29.2400   24.7267      NaN  1.648000e-01   0.6157   \n",
       "1          NaN   40.3714  40.2627    0.0000      NaN  2.500000e+07   0.8790   \n",
       "2          8.3   79.4068   0.0000   21.7234      NaN  2.957000e-01   0.1731   \n",
       "3         42.2   15.0090   6.2675   11.2025      NaN  1.466100e+00  -0.8594   \n",
       "4        252.0   99.6376  22.3897   27.3408      NaN  4.910000e-02   0.0019   \n",
       "...        ...       ...      ...       ...      ...           ...      ...   \n",
       "26819      NaN  183.0246   0.0000  266.9273      NaN  1.006320e+01   0.9371   \n",
       "26820     24.0  209.0105  34.9082   66.0515      NaN -1.465700e+00   0.5572   \n",
       "26821   3707.0  103.4521  16.0332   46.4573      NaN  3.137000e-01   0.0585   \n",
       "26822   1239.6   38.1568  37.4855    0.0000      NaN  1.140000e-02   0.1470   \n",
       "26823    106.4   21.1285   0.0000   11.0868      NaN  1.015300e+00   0.3840   \n",
       "\n",
       "       num_289  num_290  num_291  num_292  cat_98  num_293  num_294  num_295  \\\n",
       "0       6515.1   4.9348     91.0   4830.0       1      NaN  19.7434      NaN   \n",
       "1          NaN   9.0410      NaN  29782.5       2      NaN   0.0000      NaN   \n",
       "2          5.2   4.5965      NaN      0.0       3      NaN  18.7817      NaN   \n",
       "3        133.8  24.3186      NaN   3500.0       1      NaN  20.3770      NaN   \n",
       "4        906.0   3.6632      NaN      0.0       4      NaN  17.2247      NaN   \n",
       "...        ...      ...      ...      ...     ...      ...      ...      ...   \n",
       "26819      NaN   1.9942      NaN   8000.0      17      NaN   0.0000      NaN   \n",
       "26820     49.2   1.7463      NaN   1400.0     872      NaN  19.3321      NaN   \n",
       "26821   3395.0   3.5281      NaN      0.0     807      NaN  42.4703    415.0   \n",
       "26822   1781.2   9.5657      NaN      0.0      17      NaN  31.6973      NaN   \n",
       "26823    693.6  17.2752      NaN   4259.5    1143      NaN   4.2786      NaN   \n",
       "\n",
       "       num_296  cat_99  cat_100  num_297     num_298  num_299  num_300  \\\n",
       "0          3.0       1        1      NaN -7783775.80    101.0      NaN   \n",
       "1          4.0       1        1      NaN   -41999.36    124.0      NaN   \n",
       "2          NaN       1        1      NaN  -112154.12      NaN      NaN   \n",
       "3          1.0       1        1      NaN   -43685.58     37.0      NaN   \n",
       "4          2.0       1        1     14.0  -777940.94     59.0      NaN   \n",
       "...        ...     ...      ...      ...         ...      ...      ...   \n",
       "26819      1.0       1        1      NaN   -36157.12     24.0      NaN   \n",
       "26820      1.0       1        1      NaN   -15888.73     44.0      NaN   \n",
       "26821      3.0       1        3   3176.0 -1058370.91     80.0      NaN   \n",
       "26822      1.0       1        1      NaN -2893629.69     22.0      NaN   \n",
       "26823      4.0       1        1      NaN  -184601.81    126.0      NaN   \n",
       "\n",
       "       num_301  cat_101  cat_102  num_302  cat_103  num_303  num_304  cat_104  \\\n",
       "0         76.0        1        1      NaN        1   1820.9    229.0        1   \n",
       "1         32.0        1        1      NaN        1     20.0     29.0        1   \n",
       "2         77.0        1        1      NaN        1    150.6   1289.0        1   \n",
       "3         41.0        1        1      NaN        1     11.4     49.0        1   \n",
       "4         73.0        1        1  53918.0        1  59452.0    470.0        1   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "26819      9.0        1        1      NaN        1      NaN     13.0        1   \n",
       "26820     84.0        1        1     21.3        1     25.5     28.0        1   \n",
       "26821     84.0        1        1      NaN        1   4372.0    397.0        1   \n",
       "26822     25.0        1        1      NaN        1      NaN     59.0        1   \n",
       "26823     20.0        1        1      NaN        1     28.5     16.0        1   \n",
       "\n",
       "       cat_105  num_305   num_306  num_307  num_308  num_309  num_310  \\\n",
       "0            1    139.9  5126.800    286.9      NaN    139.9      NaN   \n",
       "1            1     25.0   357.200     25.0      NaN     25.0      NaN   \n",
       "2            1     99.8   107.600     99.8      NaN     20.2      NaN   \n",
       "3            1    143.1   350.600    144.5      NaN    143.1      NaN   \n",
       "4            1      7.0    56.000     17.0      NaN     17.0      NaN   \n",
       "...        ...      ...       ...      ...      ...      ...      ...   \n",
       "26819        1     72.5   113.848     72.5      NaN     85.5      NaN   \n",
       "26820        1   -149.7   544.369   -149.4      NaN    -58.3      NaN   \n",
       "26821        1    458.0  4224.080   1197.0      NaN   1432.0      NaN   \n",
       "26822        1     16.8    25.700     16.8      NaN     16.8      NaN   \n",
       "26823        1   1706.6  1706.600   1723.4      NaN    198.3      NaN   \n",
       "\n",
       "       num_311  num_312  num_313   num_314  cat_106  num_315  cat_107  \\\n",
       "0      11327.5    286.9   1014.2   559.600        1     60.1        1   \n",
       "1        168.6     25.0      0.0    25.000        1      NaN        2   \n",
       "2        209.1     99.8     13.5    20.200        1      2.4        2   \n",
       "3         56.9    144.5      0.0   143.100        1     10.9        2   \n",
       "4      60925.0     17.0     31.0    68.000        1   1017.0        1   \n",
       "...        ...      ...      ...       ...      ...      ...      ...   \n",
       "26819    181.4     72.5      0.0   113.715        2      NaN        2   \n",
       "26820    596.7   -149.4      0.0   -77.539        2     62.2        2   \n",
       "26821  14599.0   1197.0      0.0  1904.560        1   1809.0        1   \n",
       "26822   1494.9     16.8    196.2    16.800        1      2.1        1   \n",
       "26823    652.7   1723.4      0.0   198.300        1     77.9        2   \n",
       "\n",
       "       num_316  cat_108  num_317  num_318  num_319  num_320  num_321  num_322  \\\n",
       "0          NaN        1    50.00     20.0      0.0      0.0    18.03      NaN   \n",
       "1          NaN        1    53.00      1.0      1.0      0.0    64.70      NaN   \n",
       "2          NaN        1    41.50     10.0      0.0      0.0    12.39      NaN   \n",
       "3          NaN        1    54.33      4.0      0.0      0.0    25.64      NaN   \n",
       "4          NaN        1    43.33     10.0      3.0      0.0    20.14      NaN   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "26819      NaN        1    43.50      0.0      2.0      0.0    82.60      NaN   \n",
       "26820      NaN        1    41.50      8.0      1.0      0.0    56.96      NaN   \n",
       "26821     43.0        1    45.80      9.0      1.0      0.0    21.37      NaN   \n",
       "26822      NaN        2    45.00      4.0      1.0      0.0     7.14      NaN   \n",
       "26823      NaN        1    35.00      7.0      0.0      0.0    32.29      NaN   \n",
       "\n",
       "       num_323  num_324  num_325  num_326  cat_109  cat_110  num_327  num_328  \\\n",
       "0          NaN      NaN      NaN      NaN        1        1   5034.4      NaN   \n",
       "1          NaN      NaN      NaN      NaN        2        2      0.2      NaN   \n",
       "2          NaN      NaN      NaN      NaN        2        3    121.6      NaN   \n",
       "3          NaN      NaN      NaN      NaN        2        1     13.8      NaN   \n",
       "4          NaN      NaN      NaN      NaN        1        1   5421.0      NaN   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "26819      NaN      NaN      NaN      NaN        2        1    100.0      NaN   \n",
       "26820      NaN      NaN      NaN      NaN        2        2      NaN      NaN   \n",
       "26821      NaN      NaN      NaN      NaN        2        2    120.0     68.0   \n",
       "26822      NaN      NaN      NaN      NaN        2        3      0.1      NaN   \n",
       "26823      NaN      NaN      NaN      NaN        2        2    200.0      NaN   \n",
       "\n",
       "       cat_111  cat_112  num_329  num_330  num_331  num_332  num_333  num_334  \\\n",
       "0            1        1      NaN  11327.5   3303.2      NaN      NaN      NaN   \n",
       "1            1        1      NaN    168.6      0.0      1.9      NaN      NaN   \n",
       "2            1        1      NaN    209.1     68.3      NaN      NaN      NaN   \n",
       "3            1        1      NaN     56.9     97.6      0.7      NaN      NaN   \n",
       "4            2        1      NaN  60925.0   1384.0     48.0    500.0      NaN   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "26819        1        1      NaN    181.4     11.3      NaN      NaN      NaN   \n",
       "26820        1        1      NaN    596.7     52.9    268.2     50.0      NaN   \n",
       "26821        2        1      NaN  14599.0   6071.0   5372.0      NaN      NaN   \n",
       "26822        1        1      NaN   1494.9   1471.3    311.3      NaN      NaN   \n",
       "26823        1        1      NaN    652.7    195.3    904.8      NaN      NaN   \n",
       "\n",
       "       num_335  num_336  num_337  num_338  num_339  num_340     num_341  \\\n",
       "0          NaN      NaN      NaN   1014.2      NaN   6732.0  2088171.23   \n",
       "1          NaN      NaN      NaN      0.0      0.4    971.0   157527.83   \n",
       "2          NaN      NaN      NaN     13.5      NaN   3171.0     2468.00   \n",
       "3          NaN      NaN      NaN      0.0      3.3   1251.0    13654.00   \n",
       "4          NaN      NaN      NaN     31.0      NaN   4412.0   496957.00   \n",
       "...        ...      ...      ...      ...      ...      ...         ...   \n",
       "26819      NaN      NaN      NaN      0.0      0.1    272.0    69880.00   \n",
       "26820      NaN      NaN      NaN      0.0    205.2   3890.0    37770.00   \n",
       "26821      NaN      NaN      NaN      0.0   3949.0   5667.0  5727334.00   \n",
       "26822      NaN      NaN      NaN    196.2      NaN    745.0        0.00   \n",
       "26823      NaN      NaN      NaN      0.0    181.7    607.0   340318.00   \n",
       "\n",
       "       num_342  cat_113  num_343  num_344  num_345  cat_114  num_346  num_347  \\\n",
       "0          NaN        1      NaN   7933.3     68.0        1   7818.7      0.0   \n",
       "1          NaN        1      NaN    168.6     38.0        2    143.6      0.0   \n",
       "2          NaN        2      1.0    140.8     50.0        1    124.2      0.0   \n",
       "3          NaN        1      NaN    -40.7     57.0        1   -158.1      0.0   \n",
       "4          NaN        2      NaN  59541.0     64.0        1  59151.0      0.0   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "26819      NaN        2      NaN    170.1     32.0        1      NaN      0.0   \n",
       "26820      NaN        1      NaN    543.8     42.0        1    602.1      0.0   \n",
       "26821      NaN        3      NaN   8528.0     58.0        2   8317.0      0.0   \n",
       "26822      NaN        1      NaN     23.6     42.0        1      9.8      0.0   \n",
       "26823      NaN        1      NaN    457.4     50.0        1    294.8      0.0   \n",
       "\n",
       "       num_348  cat_115  num_349  cat_116  cat_117    num_350  cat_118  \\\n",
       "0          NaN        1      NaN        1        1   65084.25        1   \n",
       "1          NaN        1      NaN        1        1       0.00        1   \n",
       "2          NaN        1      NaN        1        1       0.00        1   \n",
       "3          NaN        1      NaN        1        1   16422.00        1   \n",
       "4        125.0        1      NaN        2        1  152689.12        1   \n",
       "...        ...      ...      ...      ...      ...        ...      ...   \n",
       "26819      NaN        1      NaN        1        1   11988.08        1   \n",
       "26820      NaN        1      NaN        2        1    7964.51        1   \n",
       "26821      NaN        1      NaN        3        1   21098.17        1   \n",
       "26822      NaN        1      NaN        1        1       0.00        1   \n",
       "26823      NaN        1      NaN        2        1   15708.18        1   \n",
       "\n",
       "       num_351  num_352  num_353  num_354       num_355  num_356  \\\n",
       "0       9506.6   0.0244   0.0119      2.0  2.869000e+08    211.1   \n",
       "1        148.6   0.0186   0.0186      0.0  2.500000e+07      NaN   \n",
       "2         58.5   0.3711   0.0751      0.0  1.253700e+00      NaN   \n",
       "3         45.5   0.1305   0.1291      0.0  1.445000e+08      NaN   \n",
       "4       1473.0   0.0126   0.0118      0.0  1.700000e+07      NaN   \n",
       "...        ...      ...      ...      ...           ...      ...   \n",
       "26819    181.4   0.2665   0.2663      0.0  2.013800e+00      NaN   \n",
       "26820    571.2  -0.1992  -0.0692      0.0 -1.494000e+08      NaN   \n",
       "26821  10227.0   0.0441   0.0510      0.0  1.197000e+09      NaN   \n",
       "26822   1494.9   0.0011   0.0011      0.0  1.680000e+07      NaN   \n",
       "26823    624.2   0.1598   0.0183      0.0  1.107100e+00      0.1   \n",
       "\n",
       "            num_357       num_358  num_359       num_360    num_361  num_362  \\\n",
       "0      1.399000e+08  5.055000e+00   0.0895  4.940000e-02  5034375.0      NaN   \n",
       "1      2.500000e+07  3.572000e+08   0.0000  1.482000e-01      200.0      NaN   \n",
       "2      1.253700e+00  7.970300e+00   0.0645  9.660000e-02   121622.0      NaN   \n",
       "3      1.431000e+08  3.506000e+08   0.0000  2.514900e+00    13800.0      NaN   \n",
       "4      7.000000e+06  1.806400e+00   0.0005  2.193500e+00  5420654.0      NaN   \n",
       "...             ...           ...      ...           ...        ...      ...   \n",
       "26819 -5.576900e+00  1.138480e+08   0.0000  6.268000e-01   100000.0      NaN   \n",
       "26820  1.637800e+00  5.443690e+08   0.0000 -1.299000e-01        0.0      NaN   \n",
       "26821 -1.948900e+00  4.224080e+09   0.0000  1.904560e+09        0.0      NaN   \n",
       "26822  1.680000e+07  1.309000e-01   0.1312  1.120000e-02        0.0      NaN   \n",
       "26823  1.131400e+00  1.706600e+09   0.0000  3.038000e-01        0.0      NaN   \n",
       "\n",
       "       num_363   num_364  num_365  num_366  num_367  cat_119  num_368  \\\n",
       "0          NaN  500000.0      NaN      0.0      0.0        1      NaN   \n",
       "1          NaN   35000.0      NaN      0.0      0.0        2      NaN   \n",
       "2          NaN   15000.0      NaN      0.0      0.0        2      NaN   \n",
       "3          NaN   20000.0      NaN      0.0      0.0        1      NaN   \n",
       "4          NaN   35000.0      NaN      2.0      0.0        2      NaN   \n",
       "...        ...       ...      ...      ...      ...      ...      ...   \n",
       "26819      NaN   10000.0      NaN      0.0      0.0        2      NaN   \n",
       "26820      NaN   15000.0      NaN      1.0      0.0        2      NaN   \n",
       "26821      NaN  489783.0      NaN      0.0      0.0        1      NaN   \n",
       "26822      NaN  200000.0      NaN      0.0      0.0        2      NaN   \n",
       "26823      NaN  170000.0      NaN      0.0      0.0        1     48.3   \n",
       "\n",
       "       cat_120  num_369  num_370  num_371  num_372     num_373  num_374  \\\n",
       "0            1      NaN    86.06  11728.3    28.68        0.00      NaN   \n",
       "1            1      NaN     2.01   1343.5    50.33        0.00      NaN   \n",
       "2            1      NaN    42.39    268.9    40.56        0.00      NaN   \n",
       "3            1      NaN     0.00   1106.5    57.45        0.00      NaN   \n",
       "4            1      NaN    64.80   1349.0    49.64        0.00      NaN   \n",
       "...        ...      ...      ...      ...      ...         ...      ...   \n",
       "26819        2      NaN    10.31    272.0    99.39        0.00      NaN   \n",
       "26820        2      NaN    54.38    750.0    57.85        0.00      NaN   \n",
       "26821        2      NaN    31.14  27130.0    27.96  1828724.05      NaN   \n",
       "26822        2      NaN     0.62  14299.9     6.99        0.00      NaN   \n",
       "26823        3  -1556.6    27.23  10783.2    56.06        0.00      NaN   \n",
       "\n",
       "       num_375  cat_121  num_376  num_377  num_378  num_379   num_380  \\\n",
       "0          NaN        1      NaN      NaN    139.9  11327.5  46913.20   \n",
       "1          NaN        1      NaN      NaN     25.0    168.6   1343.50   \n",
       "2          NaN        1      NaN      NaN     20.2    209.1    268.90   \n",
       "3          NaN        2      NaN      NaN    143.1     56.9   1106.50   \n",
       "4          NaN        1      NaN  53918.0      7.0  60925.0   5396.00   \n",
       "...        ...      ...      ...      ...      ...      ...       ...   \n",
       "26819      NaN        2      NaN      NaN     85.5    181.4    361.76   \n",
       "26820      NaN        2      NaN      NaN    -58.3    596.7    997.50   \n",
       "26821      NaN        1      NaN    478.0    693.0  14599.0  36082.90   \n",
       "26822      NaN        2      NaN      NaN     16.8   1494.9  14299.90   \n",
       "26823      NaN        2      NaN      NaN    198.3    652.7  10783.20   \n",
       "\n",
       "        num_381  num_382  num_383    num_384    num_385  num_386  cat_122  \\\n",
       "0       559.600   6974.5      NaN   825009.5  3469909.0      NaN        1   \n",
       "1        25.000    148.2      NaN        0.0    54239.0      NaN        1   \n",
       "2        20.200     36.2      NaN     3029.5    21352.5      NaN        1   \n",
       "3       143.100    -48.9      NaN    15403.0    16607.5      NaN        1   \n",
       "4        68.000    120.0      NaN    91980.0   537645.0      NaN        1   \n",
       "...         ...      ...      ...        ...        ...      ...      ...   \n",
       "26819   113.715    170.0      NaN        0.0    66211.0      NaN        1   \n",
       "26820   -77.539    332.5      NaN     8760.0   208488.0      NaN        1   \n",
       "26821  1904.560    855.0      NaN  1353055.0  3732855.0      NaN        1   \n",
       "26822    16.800    219.8      NaN   452454.0   545638.5      NaN        1   \n",
       "26823   198.300    250.7      NaN    38836.0   227833.0      NaN        1   \n",
       "\n",
       "       num_387  cat_123  cat_124  num_388  num_389  num_390  num_391  cat_125  \\\n",
       "0          6.0        1        1      NaN   2784.3      NaN      NaN        1   \n",
       "1          7.0        1        1      NaN    143.4      NaN      NaN        1   \n",
       "2          7.0        1        1      NaN      2.6      NaN      NaN        1   \n",
       "3          4.0        1        1      NaN   -171.9      NaN      NaN        1   \n",
       "4         11.0        1        1      NaN    196.0      NaN      NaN        2   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "26819      3.0        1        1      NaN      NaN      NaN      NaN        2   \n",
       "26820      1.0        1        1      NaN    601.6      NaN      NaN        1   \n",
       "26821      8.0        2        1      NaN      NaN      NaN      NaN        1   \n",
       "26822      2.0        1        1      NaN      9.7      NaN      NaN        1   \n",
       "26823      8.0        2        1      NaN     94.8      NaN      NaN        1   \n",
       "\n",
       "       num_392  num_393  cat_126  cat_127  cat_128  cat_129  num_394  \\\n",
       "0          NaN      0.0        1        1        1        1      NaN   \n",
       "1          NaN      2.0        1        1        1        1      NaN   \n",
       "2          NaN      0.0        1        1        1        1      NaN   \n",
       "3          NaN      0.0        1        1        1        1      NaN   \n",
       "4          NaN      5.0        1        1        1        1      6.0   \n",
       "...        ...      ...      ...      ...      ...      ...      ...   \n",
       "26819      NaN      2.0        1        1        1        1      NaN   \n",
       "26820      NaN      2.0        1        1        2        1      NaN   \n",
       "26821      NaN      1.0        1        1        2        1    568.0   \n",
       "26822      NaN      2.0        1        1        1        1      NaN   \n",
       "26823      NaN      0.0        1        1        1        1      NaN   \n",
       "\n",
       "         num_395  num_396  num_397  cat_130  num_398  num_399  num_400  \\\n",
       "0        4743.92   3758.2      NaN        1      9.4      NaN   7029.0   \n",
       "1        6419.05    148.2      NaN        1      NaN      NaN      NaN   \n",
       "2          74.09      NaN      NaN        1     72.1      NaN     98.0   \n",
       "3        5859.27     19.0      NaN        1     39.2      NaN    228.5   \n",
       "4        3161.35    331.0      NaN        1      NaN      NaN   2649.0   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "26819   32993.16      NaN      NaN        1      NaN      NaN      NaN   \n",
       "26820    2291.57     95.4      NaN        1     81.9      NaN    199.1   \n",
       "26821  151475.43   1585.0      NaN        1   1092.0      NaN   7476.0   \n",
       "26822   22353.44   1468.6      NaN        1      NaN      NaN   1813.5   \n",
       "26823  123539.25      NaN      NaN        1      2.9      NaN    777.6   \n",
       "\n",
       "       num_401  cat_131    num_402  num_403  num_404  num_405  num_406  \\\n",
       "0          NaN        1  500000.00      NaN  11728.9  11728.3   1014.2   \n",
       "1          NaN        1   35000.00      NaN   1343.5   1343.5      0.0   \n",
       "2          NaN        1   15000.00      NaN    268.9    268.9     13.5   \n",
       "3          NaN        1   20000.00      NaN   1107.8   1106.5      0.0   \n",
       "4          NaN        2  190000.00      NaN   1436.0   1349.0     31.0   \n",
       "...        ...      ...        ...      ...      ...      ...      ...   \n",
       "26819      NaN        1   10000.00      NaN    321.0    272.0      0.0   \n",
       "26820      NaN        1   15000.00      NaN    841.4    750.0      0.0   \n",
       "26821      NaN        1  489783.82      NaN  28070.0  27130.0      0.0   \n",
       "26822      NaN        1  200000.00      NaN  14299.9  14299.9    196.2   \n",
       "26823      NaN        1  170000.00      NaN  10831.5  10783.2      0.0   \n",
       "\n",
       "       num_407  num_408  num_409  num_410   num_411  num_412  num_413  \\\n",
       "0       3454.0      0.0   1014.2      0.0  46913.20  11327.5  11327.5   \n",
       "1          NaN      0.0      0.0      0.0   1343.50    168.6    168.6   \n",
       "2         10.2     79.6     13.5     79.6    268.90    209.1    209.1   \n",
       "3         24.9      0.0      0.0      0.0   1106.50     56.9     56.9   \n",
       "4        435.0      0.0     31.0      0.0   5396.00  60925.0     31.0   \n",
       "...        ...      ...      ...      ...       ...      ...      ...   \n",
       "26819      NaN    -13.0      0.0     36.0    361.76    181.4    181.4   \n",
       "26820     37.1    -91.4      0.0      0.0    997.50    596.7    596.7   \n",
       "26821   3115.0   -235.0      0.0      0.0  36082.90  14599.0      0.0   \n",
       "26822      NaN      0.0    196.2      0.0  14299.90   1494.9   1494.9   \n",
       "26823      NaN   1508.3      0.0   1556.6  10783.20    652.7    652.7   \n",
       "\n",
       "       cat_132  cat_133      num_414  num_415  num_416  cat_134  cat_135  \\\n",
       "0            1        1  28162496.65      0.0     0.07        1        1   \n",
       "1            1        1    989383.82      0.0     0.56        2        1   \n",
       "2            1        1     87444.51      0.0     0.01        3        1   \n",
       "3            1        1    297608.00      0.0     0.08        4        1   \n",
       "4            1        1   6614247.89      0.0     0.20        1        1   \n",
       "...        ...      ...          ...      ...      ...      ...      ...   \n",
       "26819        1        1    840801.82      0.0     0.09        7        1   \n",
       "26820        1        1    557554.94      0.0     0.26        5        1   \n",
       "26821        1        2  17821295.19      0.0     0.37        3        1   \n",
       "26822        1        1   7228675.60      0.0     0.00        4        1   \n",
       "26823        1        1   5277923.92      0.0     0.10        3        1   \n",
       "\n",
       "         id  gb  \n",
       "0         1   0  \n",
       "1         2   0  \n",
       "2         3   0  \n",
       "3         4   0  \n",
       "4         5   0  \n",
       "...     ...  ..  \n",
       "26819  5242   0  \n",
       "26820  4320   0  \n",
       "26821  2516   0  \n",
       "26822  4610   0  \n",
       "26823  5243   0  \n",
       "\n",
       "[26824 rows x 554 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14550bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0        0\n",
       "num_1          3796\n",
       "num_2          3871\n",
       "num_3         26761\n",
       "num_4             8\n",
       "num_5          9615\n",
       "num_6             0\n",
       "num_7             8\n",
       "cat_1             0\n",
       "num_8           332\n",
       "num_9         10460\n",
       "num_10         5638\n",
       "num_11        26788\n",
       "num_12        19008\n",
       "num_13            8\n",
       "num_14          964\n",
       "num_15            8\n",
       "num_16        15294\n",
       "num_17        16474\n",
       "num_18        26289\n",
       "num_19            0\n",
       "num_20        26379\n",
       "num_21        26762\n",
       "num_22        26823\n",
       "num_23        26824\n",
       "cat_2             0\n",
       "num_24        26823\n",
       "cat_3             0\n",
       "cat_4             0\n",
       "num_25        26025\n",
       "num_26            8\n",
       "num_27          813\n",
       "cat_5             0\n",
       "num_28            0\n",
       "num_29            8\n",
       "cat_6             0\n",
       "cat_7             0\n",
       "cat_8             0\n",
       "num_30            8\n",
       "cat_9             0\n",
       "cat_10            0\n",
       "num_31        20968\n",
       "num_32            8\n",
       "num_33            0\n",
       "num_34        19991\n",
       "num_35        24365\n",
       "num_36        22134\n",
       "num_37        24429\n",
       "num_38         3614\n",
       "num_39        22181\n",
       "num_40          786\n",
       "num_41            8\n",
       "cat_11            0\n",
       "cat_12            0\n",
       "num_42        24297\n",
       "cat_13            0\n",
       "num_43          138\n",
       "num_44        26812\n",
       "num_45            8\n",
       "cat_14            0\n",
       "cat_15            0\n",
       "num_46        26592\n",
       "num_47        26104\n",
       "num_48        26816\n",
       "num_49        26647\n",
       "num_50            0\n",
       "num_51        26731\n",
       "num_52          332\n",
       "num_53            8\n",
       "num_54        10519\n",
       "cat_16            0\n",
       "num_55           34\n",
       "num_56        26566\n",
       "num_57            8\n",
       "cat_17            0\n",
       "num_58            8\n",
       "cat_18            0\n",
       "num_59           17\n",
       "num_60          333\n",
       "num_61        26377\n",
       "num_62            8\n",
       "num_63        25119\n",
       "num_64            0\n",
       "num_65        25684\n",
       "cat_19            0\n",
       "num_66        26824\n",
       "num_67            8\n",
       "num_68            0\n",
       "num_69            0\n",
       "num_70            0\n",
       "num_71            0\n",
       "cat_20            0\n",
       "num_72            0\n",
       "num_73        26379\n",
       "num_74         9979\n",
       "num_75            8\n",
       "num_76        17014\n",
       "cat_21            0\n",
       "num_77           83\n",
       "num_78        22616\n",
       "cat_22            0\n",
       "num_79          785\n",
       "cat_23            0\n",
       "cat_24            0\n",
       "num_80         1754\n",
       "num_81         1747\n",
       "cat_25            0\n",
       "num_82        26358\n",
       "cat_26            0\n",
       "cat_27            0\n",
       "cat_28            0\n",
       "num_83            0\n",
       "num_84          333\n",
       "num_85        14750\n",
       "cat_29            0\n",
       "num_86        26820\n",
       "num_87            0\n",
       "num_88            8\n",
       "num_89        26673\n",
       "num_90        26675\n",
       "cat_30            0\n",
       "num_91        26714\n",
       "cat_31            0\n",
       "cat_32            0\n",
       "num_92            0\n",
       "num_93        23841\n",
       "num_94          819\n",
       "num_95           13\n",
       "num_96        25795\n",
       "num_97           13\n",
       "num_98        16299\n",
       "num_99        26637\n",
       "num_100       23767\n",
       "num_101       26807\n",
       "cat_33            0\n",
       "num_102         332\n",
       "num_103       26699\n",
       "num_104        8002\n",
       "num_105           8\n",
       "num_106       26375\n",
       "num_107           8\n",
       "num_108       26642\n",
       "num_109       26566\n",
       "num_110           0\n",
       "num_111           0\n",
       "num_112           8\n",
       "cat_34            0\n",
       "num_113           0\n",
       "num_114           0\n",
       "num_115           8\n",
       "cat_35            0\n",
       "num_116       25119\n",
       "num_117           0\n",
       "num_118       25186\n",
       "num_119           8\n",
       "cat_36            0\n",
       "num_120         332\n",
       "cat_37            0\n",
       "num_121       26714\n",
       "num_122           8\n",
       "cat_38            0\n",
       "cat_39            0\n",
       "num_123           8\n",
       "num_124         140\n",
       "num_125       26816\n",
       "num_126       26623\n",
       "num_127       26155\n",
       "num_128           0\n",
       "num_129         332\n",
       "num_130           8\n",
       "num_131       26608\n",
       "num_132           9\n",
       "num_133           8\n",
       "num_134           9\n",
       "cat_40            0\n",
       "cat_41            0\n",
       "cat_42            0\n",
       "num_135       26639\n",
       "cat_43            0\n",
       "num_136        4804\n",
       "num_137        5007\n",
       "num_138       26761\n",
       "num_139          15\n",
       "cat_44            0\n",
       "cat_45            0\n",
       "cat_46            0\n",
       "num_140       26620\n",
       "num_141        4045\n",
       "cat_47            0\n",
       "num_142       26642\n",
       "num_143       26635\n",
       "num_144           8\n",
       "cat_48            0\n",
       "cat_49            0\n",
       "cat_50            0\n",
       "num_145           8\n",
       "num_146       25119\n",
       "num_147        2341\n",
       "num_148         332\n",
       "num_149       23767\n",
       "num_150       26824\n",
       "cat_51            0\n",
       "num_151        2611\n",
       "cat_52            0\n",
       "num_152       26824\n",
       "cat_53            0\n",
       "num_153       19991\n",
       "num_154         737\n",
       "num_155        7109\n",
       "num_156       26818\n",
       "num_157       26814\n",
       "cat_54            0\n",
       "num_158           8\n",
       "num_159           8\n",
       "num_160       16947\n",
       "num_161           0\n",
       "num_162           8\n",
       "num_163       26212\n",
       "num_164         138\n",
       "num_165           0\n",
       "num_166       26711\n",
       "num_167           8\n",
       "num_168         138\n",
       "cat_55            0\n",
       "num_169         138\n",
       "num_170       16629\n",
       "num_171       26776\n",
       "num_172         332\n",
       "num_173          83\n",
       "cat_56            0\n",
       "num_174           0\n",
       "num_175           0\n",
       "num_176           0\n",
       "num_177       11837\n",
       "num_178           0\n",
       "num_179           0\n",
       "num_180           0\n",
       "num_181           0\n",
       "num_182       25119\n",
       "num_183           0\n",
       "num_184           8\n",
       "cat_57            0\n",
       "num_185         886\n",
       "cat_58            0\n",
       "num_186         855\n",
       "num_187         886\n",
       "num_188           8\n",
       "cat_59            0\n",
       "cat_60            0\n",
       "num_189       26797\n",
       "cat_61            0\n",
       "num_190       26810\n",
       "num_191           8\n",
       "num_192       17498\n",
       "num_193        7391\n",
       "num_194           8\n",
       "num_195       26608\n",
       "num_196         883\n",
       "num_197           8\n",
       "num_198       26787\n",
       "cat_62            0\n",
       "num_199       22474\n",
       "num_200           8\n",
       "num_201           8\n",
       "cat_63            0\n",
       "num_202           8\n",
       "cat_64            0\n",
       "cat_65            0\n",
       "cat_66            0\n",
       "num_203           8\n",
       "num_204       18288\n",
       "num_205           8\n",
       "num_206          13\n",
       "num_207       26025\n",
       "num_208       24297\n",
       "cat_67            0\n",
       "cat_68            0\n",
       "num_209           0\n",
       "num_210       26809\n",
       "cat_69            0\n",
       "cat_70            0\n",
       "num_211           8\n",
       "cat_71            0\n",
       "num_212           8\n",
       "num_213          11\n",
       "num_214         332\n",
       "num_215       26379\n",
       "num_216           8\n",
       "num_217       26762\n",
       "num_218         138\n",
       "num_219           0\n",
       "num_220           8\n",
       "num_221       20360\n",
       "num_222       26620\n",
       "num_223       26703\n",
       "num_224       26409\n",
       "num_225           8\n",
       "cat_72            0\n",
       "cat_73            0\n",
       "num_226       20740\n",
       "num_227       25082\n",
       "num_228           8\n",
       "cat_74            0\n",
       "num_229       16922\n",
       "num_230          11\n",
       "num_231        1123\n",
       "num_232         332\n",
       "cat_75            0\n",
       "num_233           8\n",
       "num_234         333\n",
       "cat_76            0\n",
       "num_235         333\n",
       "cat_77            0\n",
       "num_236       24947\n",
       "num_237        4343\n",
       "num_238           0\n",
       "cat_78            0\n",
       "num_239         855\n",
       "cat_79            0\n",
       "num_240           8\n",
       "num_241       26269\n",
       "num_242       26824\n",
       "cat_80            0\n",
       "num_243       26320\n",
       "num_244         831\n",
       "cat_81            0\n",
       "cat_82            0\n",
       "num_245       22728\n",
       "cat_83            0\n",
       "num_246           0\n",
       "num_247       25253\n",
       "num_248       26797\n",
       "num_249       26116\n",
       "num_250       26824\n",
       "num_251       22039\n",
       "num_252           8\n",
       "num_253       26739\n",
       "num_254       26418\n",
       "num_255        5113\n",
       "num_256       26814\n",
       "cat_84            0\n",
       "cat_85            0\n",
       "num_257       26797\n",
       "cat_86            0\n",
       "num_258       26824\n",
       "num_259           8\n",
       "cat_87            0\n",
       "cat_88            0\n",
       "num_260        3507\n",
       "num_261       26400\n",
       "num_262       12988\n",
       "cat_89            0\n",
       "num_263          18\n",
       "cat_90            0\n",
       "cat_91            0\n",
       "cat_92            0\n",
       "num_264           0\n",
       "num_265           0\n",
       "num_266       26824\n",
       "num_267           0\n",
       "num_268           0\n",
       "num_269           0\n",
       "num_270           0\n",
       "num_271       26789\n",
       "cat_93            0\n",
       "num_272       13577\n",
       "num_273       25119\n",
       "num_274       26418\n",
       "cat_94            0\n",
       "cat_95            0\n",
       "num_275           8\n",
       "cat_96            0\n",
       "num_276         138\n",
       "num_277           8\n",
       "num_278       21679\n",
       "num_279       26134\n",
       "cat_97            0\n",
       "num_280       25536\n",
       "num_281       26823\n",
       "num_282        3856\n",
       "num_283           0\n",
       "num_284           0\n",
       "num_285           0\n",
       "num_286       25736\n",
       "num_287           0\n",
       "num_288           0\n",
       "num_289        5036\n",
       "num_290           0\n",
       "num_291       23110\n",
       "num_292           8\n",
       "cat_98            0\n",
       "num_293       26819\n",
       "num_294           0\n",
       "num_295       25769\n",
       "num_296        1409\n",
       "cat_99            0\n",
       "cat_100           0\n",
       "num_297       25418\n",
       "num_298           8\n",
       "num_299        1409\n",
       "num_300       26548\n",
       "num_301         332\n",
       "cat_101           0\n",
       "cat_102           0\n",
       "num_302       24860\n",
       "cat_103           0\n",
       "num_303        3415\n",
       "num_304           8\n",
       "cat_104           0\n",
       "cat_105           0\n",
       "num_305           0\n",
       "num_306           0\n",
       "num_307           0\n",
       "num_308       26778\n",
       "num_309           0\n",
       "num_310       26812\n",
       "num_311           6\n",
       "num_312           0\n",
       "num_313           0\n",
       "num_314           0\n",
       "cat_106           0\n",
       "num_315        6392\n",
       "cat_107           0\n",
       "num_316       25771\n",
       "cat_108           0\n",
       "num_317        2331\n",
       "num_318           0\n",
       "num_319          11\n",
       "num_320           8\n",
       "num_321           8\n",
       "num_322       26762\n",
       "num_323       26158\n",
       "num_324       26379\n",
       "num_325       26822\n",
       "num_326       26637\n",
       "cat_109           0\n",
       "cat_110           0\n",
       "num_327        3242\n",
       "num_328       26603\n",
       "cat_111           0\n",
       "cat_112           0\n",
       "num_329       26531\n",
       "num_330           0\n",
       "num_331           0\n",
       "num_332        5590\n",
       "num_333       22616\n",
       "num_334       26409\n",
       "num_335       26735\n",
       "num_336       26782\n",
       "num_337       26522\n",
       "num_338           0\n",
       "num_339        5446\n",
       "num_340           9\n",
       "num_341           8\n",
       "num_342       26824\n",
       "cat_113           0\n",
       "num_343       22474\n",
       "num_344          81\n",
       "num_345           8\n",
       "cat_114           0\n",
       "num_346         921\n",
       "num_347          21\n",
       "num_348       26696\n",
       "cat_115           0\n",
       "num_349       26814\n",
       "cat_116           0\n",
       "cat_117           0\n",
       "num_350           8\n",
       "cat_118           0\n",
       "num_351          85\n",
       "num_352           0\n",
       "num_353           0\n",
       "num_354           8\n",
       "num_355           0\n",
       "num_356       14449\n",
       "num_357           0\n",
       "num_358           0\n",
       "num_359           0\n",
       "num_360           0\n",
       "num_361           8\n",
       "num_362       26686\n",
       "num_363       26654\n",
       "num_364           0\n",
       "num_365       26824\n",
       "num_366           8\n",
       "num_367          11\n",
       "cat_119           0\n",
       "num_368       26375\n",
       "cat_120           0\n",
       "num_369       25700\n",
       "num_370           8\n",
       "num_371           0\n",
       "num_372           8\n",
       "num_373           8\n",
       "num_374       26418\n",
       "num_375       26824\n",
       "cat_121           0\n",
       "num_376       26327\n",
       "num_377       25119\n",
       "num_378         819\n",
       "num_379           0\n",
       "num_380           0\n",
       "num_381           0\n",
       "num_382           0\n",
       "num_383       26822\n",
       "num_384           0\n",
       "num_385           0\n",
       "num_386       26810\n",
       "cat_122           0\n",
       "num_387          11\n",
       "cat_123           0\n",
       "cat_124           0\n",
       "num_388       23762\n",
       "num_389        1865\n",
       "num_390       25666\n",
       "num_391       25666\n",
       "cat_125           0\n",
       "num_392       26824\n",
       "num_393          11\n",
       "cat_126           0\n",
       "cat_127           0\n",
       "cat_128           0\n",
       "cat_129           0\n",
       "num_394       25163\n",
       "num_395          10\n",
       "num_396        6005\n",
       "num_397       26813\n",
       "cat_130           0\n",
       "num_398        8495\n",
       "num_399       26608\n",
       "num_400        1959\n",
       "num_401       26663\n",
       "cat_131           0\n",
       "num_402         332\n",
       "num_403       26668\n",
       "num_404           0\n",
       "num_405           0\n",
       "num_406           0\n",
       "num_407        3438\n",
       "num_408           0\n",
       "num_409           0\n",
       "num_410           0\n",
       "num_411           0\n",
       "num_412           0\n",
       "num_413           0\n",
       "cat_132           0\n",
       "cat_133           0\n",
       "num_414           8\n",
       "num_415           8\n",
       "num_416           8\n",
       "cat_134           0\n",
       "cat_135           0\n",
       "id                0\n",
       "gb                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# подивимось, скільки в кожній колонці пропущених значень\n",
    "pd.set_option('display.max_rows', None)\n",
    "count_na = train_df.isna().sum()\n",
    "count_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb609c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_3       99.765136\n",
       "num_11      99.865792\n",
       "num_12      70.861915\n",
       "num_16      57.016105\n",
       "num_17      61.415151\n",
       "num_18      98.005517\n",
       "num_20      98.341038\n",
       "num_21      99.768864\n",
       "num_22      99.996272\n",
       "num_23     100.000000\n",
       "num_24      99.996272\n",
       "num_25      97.021324\n",
       "num_31      78.168804\n",
       "num_34      74.526543\n",
       "num_35      90.832836\n",
       "num_36      82.515658\n",
       "num_37      91.071429\n",
       "num_39      82.690874\n",
       "num_42      90.579332\n",
       "num_44      99.955264\n",
       "num_46      99.135103\n",
       "num_47      97.315837\n",
       "num_48      99.970176\n",
       "num_49      99.340143\n",
       "num_51      99.653296\n",
       "num_56      99.038175\n",
       "num_61      98.333582\n",
       "num_63      93.643752\n",
       "num_65      95.750075\n",
       "num_66     100.000000\n",
       "num_73      98.341038\n",
       "num_76      63.428273\n",
       "num_78      84.312556\n",
       "num_82      98.262750\n",
       "num_85      54.988070\n",
       "num_86      99.985088\n",
       "num_89      99.437071\n",
       "num_90      99.444527\n",
       "num_91      99.589919\n",
       "num_93      88.879362\n",
       "num_96      96.163883\n",
       "num_98      60.762750\n",
       "num_99      99.302863\n",
       "num_100     88.603489\n",
       "num_101     99.936624\n",
       "num_103     99.533999\n",
       "num_106     98.326126\n",
       "num_108     99.321503\n",
       "num_109     99.038175\n",
       "num_116     93.643752\n",
       "num_118     93.893528\n",
       "num_121     99.589919\n",
       "num_125     99.970176\n",
       "num_126     99.250671\n",
       "num_127     97.505965\n",
       "num_131     99.194751\n",
       "num_135     99.310319\n",
       "num_138     99.765136\n",
       "num_140     99.239487\n",
       "num_142     99.321503\n",
       "num_143     99.295407\n",
       "num_146     93.643752\n",
       "num_149     88.603489\n",
       "num_150    100.000000\n",
       "num_152    100.000000\n",
       "num_153     74.526543\n",
       "num_156     99.977632\n",
       "num_157     99.962720\n",
       "num_160     63.178497\n",
       "num_163     97.718461\n",
       "num_166     99.578735\n",
       "num_170     61.992991\n",
       "num_171     99.821056\n",
       "num_182     93.643752\n",
       "num_189     99.899344\n",
       "num_190     99.947808\n",
       "num_192     65.232627\n",
       "num_195     99.194751\n",
       "num_198     99.862064\n",
       "num_199     83.783179\n",
       "num_204     68.177751\n",
       "num_207     97.021324\n",
       "num_208     90.579332\n",
       "num_210     99.944080\n",
       "num_215     98.341038\n",
       "num_217     99.768864\n",
       "num_221     75.902177\n",
       "num_222     99.239487\n",
       "num_223     99.548911\n",
       "num_224     98.452878\n",
       "num_226     77.318819\n",
       "num_227     93.505816\n",
       "num_229     63.085297\n",
       "num_236     93.002535\n",
       "num_241     97.930957\n",
       "num_242    100.000000\n",
       "num_243     98.121086\n",
       "num_245     84.730092\n",
       "num_247     94.143305\n",
       "num_248     99.899344\n",
       "num_249     97.360573\n",
       "num_250    100.000000\n",
       "num_251     82.161497\n",
       "num_253     99.683120\n",
       "num_254     98.486430\n",
       "num_256     99.962720\n",
       "num_257     99.899344\n",
       "num_258    100.000000\n",
       "num_261     98.419326\n",
       "num_266    100.000000\n",
       "num_271     99.869520\n",
       "num_272     50.615121\n",
       "num_273     93.643752\n",
       "num_274     98.486430\n",
       "num_278     80.819415\n",
       "num_279     97.427677\n",
       "num_280     95.198330\n",
       "num_281     99.996272\n",
       "num_286     95.943931\n",
       "num_291     86.154190\n",
       "num_293     99.981360\n",
       "num_295     96.066955\n",
       "num_297     94.758425\n",
       "num_300     98.971071\n",
       "num_302     92.678199\n",
       "num_308     99.828512\n",
       "num_310     99.955264\n",
       "num_316     96.074411\n",
       "num_322     99.768864\n",
       "num_323     97.517149\n",
       "num_324     98.341038\n",
       "num_325     99.992544\n",
       "num_326     99.302863\n",
       "num_328     99.176111\n",
       "num_329     98.907695\n",
       "num_333     84.312556\n",
       "num_334     98.452878\n",
       "num_335     99.668208\n",
       "num_336     99.843424\n",
       "num_337     98.874143\n",
       "num_342    100.000000\n",
       "num_343     83.783179\n",
       "num_348     99.522815\n",
       "num_349     99.962720\n",
       "num_356     53.865941\n",
       "num_362     99.485535\n",
       "num_363     99.366239\n",
       "num_365    100.000000\n",
       "num_368     98.326126\n",
       "num_369     95.809723\n",
       "num_374     98.486430\n",
       "num_375    100.000000\n",
       "num_376     98.147182\n",
       "num_377     93.643752\n",
       "num_383     99.992544\n",
       "num_386     99.947808\n",
       "num_388     88.584849\n",
       "num_390     95.682970\n",
       "num_391     95.682970\n",
       "num_392    100.000000\n",
       "num_394     93.807784\n",
       "num_397     99.958992\n",
       "num_399     99.194751\n",
       "num_401     99.399791\n",
       "num_403     99.418431\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тепер подивимось тільки ті колонки, в яких частка пропущених значень більше 50% \n",
    "pd.set_option('display.max_rows', None)\n",
    "na_percentage = count_na[count_na / len(train_df) > 0.5] / len(train_df) * 100\n",
    "na_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b67c4b5",
   "metadata": {},
   "source": [
    "Якщо подивитись на ці колонки вище, то помітно, що є багато колонок із пропусками в районі 90%.. але спершу виведемо к-сть колонок із пропущеними більше 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "255acf66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "na_percentage.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20cea3e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тепер виведемо к-сть колонок з пропущеними більше 90%\n",
    "na_percentage1 = count_na[count_na / len(train_df) > 0.9] / len(train_df) * 100\n",
    "na_percentage1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c0061c",
   "metadata": {},
   "source": [
    "Більша частина колонок, де % пропущених більше 50 виявилась більше 90 - тобто даних у них дуже мало, потрібно визначитись, що ми будемо робити з цими пропусками далі. Потрібно зʼясувати, чи є лінійна залежність між змінними та цільовою змінною.\n",
    "\n",
    "Для цього спробуємо вивести візуалізацію кореляції між змінними та цільовою. Так як змінних на даному етапі у нас багато, то варто спробувати вивести для зручності топ-50 колонок, у яуих кореляція із цільовою найбільша.\n",
    "Для цього відберемо на початку числові стовпці  окремота застосуємо one-hot encoding до категоріальних змінних. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ae5a3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3051176061.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = train_df[numerical_columns]\n",
      "/Users/vitamajstrenko/anaconda3/lib/python3.10/site-packages/numpy/lib/function_base.py:2845: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  c = cov(x, y, rowvar, dtype=dtype)\n",
      "/Users/vitamajstrenko/anaconda3/lib/python3.10/site-packages/numpy/lib/function_base.py:2704: RuntimeWarning: divide by zero encountered in divide\n",
      "  c *= np.true_divide(1, fact)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Вибір числових стовпців\n",
    "numerical_columns = [col for col in train_df.columns if col.startswith('num_')]\n",
    "\n",
    "# Застосування one-hot encoding до категоріальних змінних\n",
    "encoded_data = pd.get_dummies(train_df.drop(numerical_columns, axis=1), prefix_sep='_', drop_first=True)\n",
    "\n",
    "# Додавання числових стовпців до one-hot encoded даних\n",
    "encoded_data[numerical_columns] = train_df[numerical_columns]\n",
    "\n",
    "# Обчислення кореляції кожної колонки з цільовою змінною\n",
    "correlation_gb = encoded_data.corrwith(train_df['gb']).abs().sort_values(ascending=False)\n",
    "\n",
    "# Вибір топ-50 колонок з найбільшими значеннями кореляції\n",
    "top_50_correlated = correlation_gb[:50]\n",
    "\n",
    "# Візуалізація топ-50 кореляцій\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.barplot(x=top_50_correlated.values, y=top_50_correlated.index)\n",
    "plt.xlabel('Correlation')\n",
    "plt.ylabel('Columns')\n",
    "plt.title('Top 50 Correlations with Target Variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ab94d1",
   "metadata": {},
   "source": [
    "З візуалізації бачимо, що ми не прибрали цільову змінну - і тому кореляція 1.\n",
    "\n",
    "в усіх інших -  кореляція дуже слабка з цільовою не більше 0.2 .  \n",
    "Тому пропущені значення заповнимо медіаною трохи пізніше."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1cbae",
   "metadata": {},
   "source": [
    "Тепер перходимо до розділення нашого датасету на 3 вибірки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f11fbb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Розмір тренувальної вибірки: (18776, 388)\n",
      "Розмір валідаційної вибірки: (4024, 388)\n",
      "Розмір тестової вибірки: (4024, 388)\n"
     ]
    }
   ],
   "source": [
    "# Визначення ознак та цільової змінної\n",
    "features = encoded_data.drop('gb', axis=1)\n",
    "target = encoded_data['gb']\n",
    "threshold = 0.5  # Порогове значення для відсотку пропущених значень\n",
    "\n",
    "# Видалення колонок з features, де % пропусків більше 50\n",
    "columns_to_drop_features = count_na[count_na / len(train_df) > threshold].index\n",
    "features = features.drop(columns_to_drop_features, axis=1)\n",
    "\n",
    "\n",
    "# Розділення на тренувальну, валідаційну та тестову вибірки зі співвідношенням 70-15-15\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Перевірка розмірів вибірок\n",
    "print(\"Розмір тренувальної вибірки:\", X_train.shape)\n",
    "print(\"Розмір валідаційної вибірки:\", X_val.shape)\n",
    "print(\"Розмір тестової вибірки:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d50355",
   "metadata": {},
   "source": [
    "## preprocessing тренувальних даних¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d033e7e",
   "metadata": {},
   "source": [
    "Підхід: \n",
    "1) обираю лише ті стовпці, де є щонайменше 50% заповнених значень\n",
    "2)пропущені заповнюю медіаною\n",
    "3)далі здійснюю кодування категоріальних змінних за допомогою one-hot encoding до обох наборів даних.\n",
    "4) перевіряю наявність однакових стовпців у тренувальному, валідаційному та тестовому наборах даних\n",
    "5) перевіряю порядок стовпців,чи у  тестовому наборі відповідає порядку у тренувальному наборі"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c6ae1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стовпці, які є в тренувальній, але відсутні в валідаційній: set()\n",
      "Стовпці, які є в тренувальній, але відсутні в тестовій: set()\n",
      "Порядок стовпців у тренувальній та тестовій вибірках співпадає.\n"
     ]
    }
   ],
   "source": [
    "# Заповнення пропущених значень медіаною\n",
    "X_train_filled = X_train.fillna(X_train.median())\n",
    "# Перевірка наявності однакових стовпців\n",
    "columns_train = list(X_train_filled.columns)\n",
    "columns_val = list(X_val.columns)\n",
    "columns_test = list(X_test.columns)\n",
    "\n",
    "print(\"Стовпці, які є в тренувальній, але відсутні в валідаційній:\", set(columns_train) - set(columns_val))\n",
    "print(\"Стовпці, які є в тренувальній, але відсутні в тестовій:\", set(columns_train) - set(columns_test))\n",
    "\n",
    "\n",
    "# Перевірка порядку стовпців\n",
    "if list(X_train_filled.columns) == list(X_test.columns):\n",
    "    print(\"Порядок стовпців у тренувальній та тестовій вибірках співпадає.\")\n",
    "else:\n",
    "    print(\"Порядок стовпців у тренувальній та тестовій вибірках не співпадає.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984dbd4f",
   "metadata": {},
   "source": [
    "# preprocessing валідаційних даних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfd3c024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Порядок стовпців у тренувальній та валідаційній вибірках співпадає.\n"
     ]
    }
   ],
   "source": [
    "# Заповнення пропущених значень медіаною\n",
    "X_val_filled = X_val.fillna(X_train.median())\n",
    "\n",
    "if list(X_train_filled.columns) == list(X_val_filled.columns):\n",
    "    print(\"Порядок стовпців у тренувальній та валідаційній вибірках співпадає.\")\n",
    "else:\n",
    "    print(\"Порядок стовпців у тренувальній та валідаційній вибірках не співпадає.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957533c9",
   "metadata": {},
   "source": [
    "# preprocessing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39a64937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стовпці, які є в тренувальній, але відсутні в тестовій: set()\n",
      "Порядок стовпців у тренувальній та тестовій вибірках співпадає.\n"
     ]
    }
   ],
   "source": [
    "X_test_filled = X_test.fillna(X_train.median())\n",
    "columns_test = list(X_test_filled.columns)\n",
    "\n",
    "print(\"Стовпці, які є в тренувальній, але відсутні в тестовій:\", set(columns_train) - set(columns_test))\n",
    "\n",
    "\n",
    "if list(X_train_filled.columns) == list(X_test_filled.columns):\n",
    "    print(\"Порядок стовпців у тренувальній та тестовій вибірках співпадає.\")\n",
    "else:\n",
    "    print(\"Порядок стовпців у тренувальній та тестовій вибірках не співпадає.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f4344a",
   "metadata": {},
   "source": [
    "Після обробки даних, переходимо до вибору моделі. Я попередньо відібрала 5 для задачі бінарної класифікації. Пробую натренувати 5 моделей на тренувальній вибірці, та оцінити її точність на валідаційній."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0708cfc",
   "metadata": {},
   "source": [
    "# Вибір моделі\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "779c303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# деякі обрані моделі можуть потребувати масштабування, тому створюємо масштабовані змінні\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_filled)\n",
    "X_val_scaled = scaler.transform(X_val_filled)\n",
    "X_test_scaled = scaler.transform(X_test_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd71651d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Створення списку моделей\n",
    "models = [\n",
    "    ('Naive Bayes', GaussianNB()),\n",
    "    ('Support Vector Machine', SVC()),\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier()),\n",
    "    ('Logistic Regression', LogisticRegression(max_iter=13000))\n",
    "]\n",
    "\n",
    "# Навчання та оцінка моделей\n",
    "results = []\n",
    "\n",
    "for name, model in models:\n",
    "    if name in ['Naive Bayes', 'Decision Tree']:\n",
    "        # Використання немасштабованих даних\n",
    "        X_train_model = X_train_filled\n",
    "        X_val_model = X_val_filled\n",
    "    else:\n",
    "        # Використання масштабованих даних\n",
    "        X_train_model = X_train_scaled\n",
    "        X_val_model = X_val_scaled\n",
    "    \n",
    "    # Навчання моделі\n",
    "    model.fit(X_train_model, y_train)\n",
    "    \n",
    "    # Оцінка моделі на навчальних даних\n",
    "    train_score = model.score(X_train_model, y_train)\n",
    "    \n",
    "    # Оцінка моделі на валідаційних даних\n",
    "    val_score = model.score(X_val_model, y_val)\n",
    "    \n",
    "    # Розрахунок RMSE на навчальних даних\n",
    "    train_predictions = model.predict(X_train_model)\n",
    "    train_rmse = mean_squared_error(y_train, train_predictions, squared=False)\n",
    "    \n",
    "    # Розрахунок RMSE на валідаційних даних\n",
    "    val_predictions = model.predict(X_val_model)\n",
    "    val_rmse = mean_squared_error(y_val, val_predictions, squared=False)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train Accuracy': train_score,\n",
    "        'Validation Accuracy': val_score,\n",
    "        'Train RMSE': train_rmse,\n",
    "        'Validation RMSE': val_rmse\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ab15497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Naive Bayes\n",
      "Train Accuracy: 0.1253\n",
      "Validation Accuracy: 0.1096\n",
      "Train RMSE: 0.9353\n",
      "Validation RMSE: 0.9436\n",
      "\n",
      "Model: Support Vector Machine\n",
      "Train Accuracy: 0.9800\n",
      "Validation Accuracy: 0.9831\n",
      "Train RMSE: 0.1415\n",
      "Validation RMSE: 0.1300\n",
      "\n",
      "Model: Decision Tree\n",
      "Train Accuracy: 1.0000\n",
      "Validation Accuracy: 0.9858\n",
      "Train RMSE: 0.0000\n",
      "Validation RMSE: 0.1190\n",
      "\n",
      "Model: Random Forest\n",
      "Train Accuracy: 1.0000\n",
      "Validation Accuracy: 0.9933\n",
      "Train RMSE: 0.0000\n",
      "Validation RMSE: 0.0819\n",
      "\n",
      "Model: Logistic Regression\n",
      "Train Accuracy: 0.9812\n",
      "Validation Accuracy: 0.9838\n",
      "Train RMSE: 0.1371\n",
      "Validation RMSE: 0.1271\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Виведення результатів\n",
    "for result in results:\n",
    "    model_name = result['Model']\n",
    "    train_accuracy = result['Train Accuracy']\n",
    "    val_accuracy = result['Validation Accuracy']\n",
    "    train_rmse = result['Train RMSE']\n",
    "    val_rmse = result['Validation RMSE']\n",
    "    \n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd15dfd2",
   "metadata": {},
   "source": [
    "На підставі наведених результатів, модель \"Support Vector Machine\" виявляється найоптимальнішою серед розглянутих моделей - бо моделі з вищою точністю - ймовірно, схильні до перенавчанняю. Маючи високу точність навчання (Train Accuracy: 0.98) і точність на валідаційних даних (Validation Accuracy: 0.9831), а також низьке значення середньоквадратичного відхилення для навчальних даних (Train RMSE: 0.1415) і валідаційних даних (Validation RMSE: 0.13), ця модель показує стабільні та точні результати на обох наборах даних."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27ca28ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine\n",
      "Train Accuracy: 0.9812\n",
      "Validation Accuracy: 0.9831\n",
      "Train RMSE: 0.1371\n",
      "Validation RMSE: 0.1300\n",
      "Precision: 1.0000\n",
      "Recall: 0.0286\n",
      "F1-score: 0.0556\n"
     ]
    }
   ],
   "source": [
    "# Навчання моделі на тренувальних даних\n",
    "# Створення моделі Support Vector Machine\n",
    "svm_model = SVC()\n",
    "\n",
    "# Навчання моделі на тренувальних даних\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Оцінка моделі на валідаційних даних\n",
    "val_accuracy = svm_model.score(X_val_scaled, y_val)\n",
    "val_predictions = svm_model.predict(X_val_scaled)\n",
    "val_rmse = mean_squared_error(y_val, val_predictions, squared=False)\n",
    "val_precision = precision_score(y_val, val_predictions)\n",
    "val_recall = recall_score(y_val, val_predictions)\n",
    "val_f1 = f1_score(y_val, val_predictions)\n",
    "\n",
    "print(\"Support Vector Machine\")\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "print(f\"Precision: {val_precision:.4f}\")\n",
    "print(f\"Recall: {val_recall:.4f}\")\n",
    "print(f\"F1-score: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d79da162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive examples: 2\n",
      "Negative examples: 4022\n"
     ]
    }
   ],
   "source": [
    "# Отримання прогнозів моделі на валідаційних даних\n",
    "val_predictions = svm_model.predict(X_val_scaled)\n",
    "\n",
    "# Підрахунок кількості позитивних та негативних значень\n",
    "positive_count = len(val_predictions[val_predictions == 1])\n",
    "negative_count = len(val_predictions[val_predictions == 0])\n",
    "\n",
    "print(\"Positive examples:\", positive_count)\n",
    "print(\"Negative examples:\", negative_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ad342",
   "metadata": {},
   "source": [
    "схоже, модель некоректно відпрацьовує - оскільки не може побачити позитивні значення, спробую модифікувати модель, знайду оптимальні параметри"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ba885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Створення об'єкта SVM\n",
    "svm = SVC()\n",
    "\n",
    "# Створення гіперпараметрів для пошуку\n",
    "param_grid = {'C': [0.1, 1, 10, 100],\n",
    "              'gamma': [0.1, 0.01, 0.001, 0.0001]}\n",
    "\n",
    "# Створення об'єкта GridSearchCV\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "\n",
    "# Пошук оптимальних параметрів\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "# Виведення найкращих параметрів\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "\n",
    "# Оцінка моделі на тренувальних та валідаційних даних з оптимальними параметрами\n",
    "train_accuracy = grid_search.best_estimator_.score(X_train_scaled, y_train)\n",
    "val_accuracy = grid_search.best_estimator_.score(X_val_scaled, y_val)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, grid_search.best_estimator_.predict(X_train_scaled)))\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val, grid_search.best_estimator_.predict(X_val_scaled)))\n",
    "\n",
    "print(\"Support Vector Machine (Optimized)\")\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Validation RMSE: {val_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0f583adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=100, gamma=0.001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=100, gamma=0.001)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=100, gamma=0.001)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тренуємо модель тепер з новими параметрами\n",
    "svm = SVC(C=100, gamma=0.001)\n",
    "\n",
    "# Навчання моделі на тренувальних даних\n",
    "svm.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd224c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine (Optimized)\n",
      "Validation Accuracy: 0.9925\n",
      "Validation RMSE: 0.0863\n"
     ]
    }
   ],
   "source": [
    "# Оцінка моделі на валідаційних даних\n",
    "validation_accuracy = svm.score(X_val_scaled, y_val)\n",
    "validation_rmse = np.sqrt(mean_squared_error(y_val, svm.predict(X_val_scaled)))\n",
    "\n",
    "print(\"Support Vector Machine (Optimized)\")\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n",
    "print(f\"Validation RMSE: {validation_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "975a804f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine (Optimized)\n",
      "Train Accuracy: 0.9996\n",
      "Test Accuracy: 0.9916\n",
      "Train RMSE: 0.0206\n",
      "Test RMSE: 0.0919\n"
     ]
    }
   ],
   "source": [
    "# Створення об'єкта SVM з оптимальними параметрами\n",
    "svm = SVC(C=100, gamma=0.001)\n",
    "\n",
    "# Навчання моделі на тренувальних даних\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Оцінка моделі на тренувальних та тестових даних\n",
    "train_accuracy = svm.score(X_train_scaled, y_train)\n",
    "test_accuracy = svm.score(X_test_scaled, y_test)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, svm.predict(X_train_scaled)))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, svm.predict(X_test_scaled)))\n",
    "\n",
    "print(\"Support Vector Machine (Optimized)\")\n",
    "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d213b7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Examples on Test Data:\n",
      "Positive examples: 68\n",
      "Negative examples: 3956\n"
     ]
    }
   ],
   "source": [
    "# Передбачення на тестовій вибірці\n",
    "y_pred = svm.predict(X_test_scaled)\n",
    "\n",
    "# Підрахунок кількості позитивних і негативних значень\n",
    "positive_examples = np.sum(y_pred == 1)\n",
    "negative_examples = np.sum(y_pred == 0)\n",
    "\n",
    "print(\"Predicted Examples on Test Data:\")\n",
    "print(f\"Positive examples: {positive_examples}\")\n",
    "print(f\"Negative examples: {negative_examples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fee4bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual Examples on Test Data:\n",
      "Positive examples: 92\n",
      "Negative examples: 3932\n"
     ]
    }
   ],
   "source": [
    "y_test_actual = y_test\n",
    "\n",
    "print(\"Actual Examples on Test Data:\")\n",
    "print(f\"Positive examples: {np.sum(y_test_actual == 1)}\")\n",
    "print(f\"Negative examples: {np.sum(y_test_actual == 0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39bf88da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# тепер працюємо з файлом delay - додаємо пусту колонку\n",
    "delay_df['gb'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7481b470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n",
      "/var/folders/r7/mpd7z_ms31j9b6hwvjlnymsh0000gn/T/ipykernel_2281/3492728916.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_data[numerical_columns] = delay_df[numerical_columns]\n"
     ]
    }
   ],
   "source": [
    "# Вибір числових стовпців\n",
    "numerical_columns = [col for col in delay_df.columns if col.startswith('num_')]\n",
    "\n",
    "# Застосування one-hot encoding до категоріальних змінних\n",
    "encoded_data = pd.get_dummies(delay_df.drop(numerical_columns, axis=1), prefix_sep='_', drop_first=True)\n",
    "\n",
    "# Додавання числових стовпців до one-hot encoded даних\n",
    "encoded_data[numerical_columns] = delay_df[numerical_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb3c64f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vitamajstrenko/anaconda3/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but SVC was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nSVC does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Заповнення колонки 'gb' за допомогою натренованої моделі\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predicted_gb \u001b[38;5;241m=\u001b[39m \u001b[43msvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_delay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Оновлення значень колонки 'gb' в encoded_data_delay\u001b[39;00m\n\u001b[1;32m      5\u001b[0m encoded_data_delay[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgb\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m predicted_gb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py:820\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    818\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py:433\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;124;03m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_for_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    434\u001b[0m     predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py:613\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    610\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel):\n\u001b[0;32m--> 613\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39misspmatrix(X):\n\u001b[1;32m    623\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 565\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    918\u001b[0m         )\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 921\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     )\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nSVC does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# Заповнення колонки 'gb' за допомогою натренованої моделі\n",
    "predicted_gb = svm.predict(features_delay)\n",
    "\n",
    "# Оновлення значень колонки 'gb' в encoded_data_delay\n",
    "encoded_data_delay['gb'] = predicted_gb\n",
    "\n",
    "# Визначення ознак та цільової змінної\n",
    "features = encoded_data.drop('gb', axis=1)\n",
    "target = encoded_data['gb']\n",
    "threshold = 0.5  # Порогове значення для відсотку пропущених значень\n",
    "\n",
    "# Видалення колонок з features, де % пропусків більше 50\n",
    "columns_to_drop_features = count_na[count_na / len(train_df) > threshold].index\n",
    "features = features.drop(columns_to_drop_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b5eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Збереження передбачених значень та нової колонки 'gb' у файлі\n",
    "encoded_data_delay.to_csv('predicted_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1dd97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307ad9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472e8951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac940c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec3c8b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
